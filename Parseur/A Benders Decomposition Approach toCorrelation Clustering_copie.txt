<informations> <informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion><titre></titre> <titre></titre> <titre></titre> <titre></titre> <titre></titre> <titre></titre> <titre></titre> <titre></titre> <titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion></informations><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion><titre></titre> <auteur></auteur><abstract>AbstractWe tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). Wereformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Bendersdecomposition formulation has many subproblems, each associated with a node inthe CC instance’s graph, which can be solved in parallel. Each Benders subproblemenforces the cycle inequalities corresponding to edges with negative (repulsive)weights attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and, in contrast to previous cutting planeapproaches, theoretically allows for massive parallelization.</abstract><introduction>IntroductionMany computer vision tasks involve partitioning (clustering) a set of observations into unique entities.A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is definedon a sparse graph with real valued edge weights, where nodes correspond to observations andweighted edges describe the affinity between pairs of nodes.For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels andedges indicate adjacency between superpixels. The weight of the edge between a pair of superpixelsrelates to the probability, as defined by a classifier, that the two superpixels belong to the same groundtruth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .The magnitude of the weight is a function of the confidence of the classifier.The CC cost function sums up the weights of the edges separating connected components (referredto as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph intoentities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emergesnaturally as a function of the edge weights, rather than requiring an additional search over somemodel order parameter describing the number of clusters (entities) [37].Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CCproblems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linearprogramming with cutting planes. They do not scale easily to large CC problem instances and are notPreprint. Under review.easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization inCC for domains, where massively parallel computation could be employed.In this paper we apply the classic Benders decomposition from operations research [10] to CC forcomputer vision. Benders decomposition is commonly applied in operations research to solve mixedinteger linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. Theblock structure requires that no row of the constraint matrix of the MILP contains variables frommore than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimizationproceeds with the master problem solving optimization over its variables. The subsequent solutionof the subproblems can be done in parallel and provides primal/dual solutions over their variablesconditioned on the solution to the master problem. The dual solutions to the subproblems provideconstraints to the master problem. Optimization continues until no further constraints are added tothe master problem.Benders decomposition is an exact MILP programming solver, but can be intuitively understood asa coordinate descent procedure, iterating between the master problem and the subproblems. Here,solving the subproblems not only provides a solution for their variables, but also a lower bound in theform of a hyper-plane over the master problem’s variables. This lower bound is tight at the currentsolution to the master problem.Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with analternative (often random) objective under the hard constraint of optimality (possibly within a factor)regarding the original objective of the subproblem.Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. Thisallows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].</introduction> <corps>2Related WorkCorrelation clustering has been successfully applied to multiple problems in computer vision includingimage segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond tosuperpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cutstrategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order costterms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimizationscheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relieson random sampling and only provides optimality bounds.Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computervision. They introduce a column generation [16, 6] approach, where the pricing problem correspondsto finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum costperfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentationin Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.[39], Andres et al. [3].Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usuallyaddressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant inpractice whenever the optimal solution is out of reach, but they do not provide any guarantees on thequality of the solution.Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodescorrespond to detections of objects and edges are associated with probabilities of co-association.Thework of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulatemulti-person pose estimation using CC augmented with node labeling.Our work is derived from the classical work in operations research on Benders decomposition [10, 11,15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solvesa mixed integer linear program over a set of fixed charge variables (opening links) and a larger set offractional variables (flows of commodities from facilities to customers in a network) associated with2constraints. Benders decomposition reformulates optimization so as to use only the integer variablesand converts the fractional variables into constraints. These constraints are referred to as Bendersrows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by theuse of MWR [23], which are more binding than the standard Benders rows.Benders decomposition has recently been introduced to computer vision (though not for CC), for thepurpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation ismodeled so as to admit efficient optimization, using column generation and Benders decompositionjointly. The application of Benders decomposition in our paper is distinct regarding the problemdomain, the underlying integer program and the structure of the Benders subproblems.3Standard Correlation Clustering FormulationIn this section, we review the standard optimization formulation for CC [1], which corresponds to agraph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the followingbinary edge labeling problem.Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. Alabel xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and iszero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edgelabel x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:minx∈{0,1}|E|s.t.X(vi ,vj )∈E −XX−φvi vj (1 − xvi vj ) +φ vi vj x vi vj(CC1 )(vi ,vj )∈E +xvi vj ≥ xvic vjc∀c ∈ C,(1)(vi ,vj )∈Ec+where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) isthe edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge(vi , vj ) with xvi vj = 1 as a cut edge.The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforcesthe labeling x to decompose G such that cut edges are exactly those edges that straddle distinctcomponents. We refer to the constraints in Eq. (1) as cycle inequalities.Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortestpath between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If theˆ Thecorresponding path has total weight less than xvi vj , the corresponding constraint is added to C.LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violatedcycle inequalities exist, after which the ILP must be solved in each iteration.We should note that earlier work in CC for computer vision did not require that cycle inequalitiescontain exactly one member of E − , which is on the right hand side of Eq. (1). It is established withLemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)or its LP relaxation.In this section, we reviewed the baseline approach for solving CC in the computer vision community.In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on thespecific solver of Andres et al. [1].34Benders Decomposition for Correlation ClusteringIn this section, we introduce a novel approach to CC using Benders decomposition (referred to asBDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with membersS ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to asthe root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblemwith root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with rootvs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+to denote the subset of E + adjacent to vs .In this section, we assume that we are provided with S, which can be produced greedily or using anLP/ILP solver.Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides thecost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) inE + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, whichis based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is thenumber of edges in the subproblem s.XXX(CC1 )(CC2 ) :min−φvi vj (1 − xvi vj ) +φ vi vj x vi vj +Q(φ, s, x),x∈{0,1}|E|(vi ,vj )∈E −(vi ,vj )∈E +s∈S(CC2 )where Q(φ, s, x) is defined as follows.Q(φ, s, x)=minsx ∈{0,1}s.t.X|s|−φvi vj (1 − xsvi vj ) +(vi ,vj )∈Es−XXφvi vj xsvi vj(2)(vi ,vj )∈E +xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .(vi ,vj )∈Ec+We now construct a solution x∗ = {x∗vi vj , (xs∗vi vj )s∈S } for which Eq. (CC2 ) is minimized and allcycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceedas follows.Mx∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ SMx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E + .(3)(4)The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗vi vj andis defined as follows.1, if (vi , vj ) ∈ Es−xs∗=(5)vi vj0, otherwise.In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗vi vj )s∈S } is no greater than that of{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for alls ∈ S.It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 forall s ∈ S.Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is2-colorable. This is because any partition xs can be altered without increasing its cost, by mergingconnected components that are adjacent to one another, not including the root node vs . Note, thatmerging any pair of such components, does not increase the cost, since those components are notseparated by negative weight edges in subproblem s and so the result is still a partition.Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the nodelabeling formulation of min-cut, with the notation below.4We indicate with mv = 1 that node v ∈ V is not in the component associated with the root ofsubproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let(1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in xsf vi vj =(6)1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added toQ(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all(vi , vj ) ∈ Es− .Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variablesψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negativeto ensure that there exists an optimizing solution for f, m which is binary. This is a consequence ofthe optimization being totally unimodular, given that x is binary. Total unimodularity is a knownproperty of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.XXQ(φ, s, x) = sminφvi vj fvsi vj −φvs v fvss v(7)fv v ≥0i j(vi ,vj )∈E +mv ≥0(vs ,v)∈Es−λ−vi vj:mvi − mvj ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),λ+vi vj:mvj − mvi ≤ xvi vj + fvsi vj∀(vi , vj ) ∈ (E + \ Es+ ),ψv−:xvs v − fvss v ≤ mv∀(vs , v) ∈ Es− ,ψv+:mv ≤ xvs v + fvss v∀(vs , v) ∈ Es+ ,This yields to the corresponding dual subproblem.X+max −(λ−vi vj + λvi vj )xvi vj +λ≥0ψ≥0s.t.ψv+iXψv− xvs v −(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )Xψv+ xvs v(8)(vs ,v)∈Es+1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+XX+(λ−vi vj − λvi vj ) +vj(vi ,vj )∈(E + \Es+ )φ vi vj−(λ+vj vi − λvj vi ) ≥ 0∀vi ∈ V − vsvj(vj ,vi )∈(E + \Es+ )−φvs v − ψv− ≥ 0φvs v − ψv+ ≥ 0+− (λ−vi v j + λ vi vj ) ≥ 0∀(vs , v) ∈ Es−∀(vs , v) ∈ Es+∀(vi , vj ) ∈ (E + \ Es+ ).In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returnsone if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note thatany dual feasible solution for the dual problem (8) describes an affine function of x, which is a tightlower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with thexvi vj term.+−(λ−if (vi , vj ) ∈ (E + \ Es+ )vi vj + λvi vj ),−ψv+j ,if (vi , vj ) ∈ Es+ωvzi vj =ψv−j ,if (vi , vj ) ∈ Es−0,if (vi , vj ) ∈ (E − \ Es− ).We denote the set of all dual feasible solutions across s P∈ S as Z, with z ∈ Z. Observe, that toenforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.We formulate CC as optimization using Z below.XX(CC2 )(CC3 ) = minφ vi vj x vi vj −(1 − xvi vj )φvi vj(CC3 )x∈{0,1}|E|s.t.X(vi ,vj )∈E −(vi ,vj )∈E +xvi vj ωvzi vj ≤ 0∀z ∈ Z(vi ,vj )∈E5Algorithm 1 Benders Decomposition for CC (BDCC)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:4.1Ẑ = {}done_LP = Falserepeatx = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=Truedid_add = Falsefor s ∈ S doif ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj thenz1 = Get Benders row via Eq (8).z2 = Get MWR via Sec. 5.Ẑ = Ẑ ∪ z1 ∪ z2did_add = Trueend ifend forif did_add=False thendone_LP = Trueend ifuntil did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ EReturn xCutting Plane OptimizationOptimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions acrosssubproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting planeapproach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ asthe empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as themaster problem) and generating new Benders rows until no violated constraints exist.This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforceintegrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if sis associated with a violated cycle inequality, which we determine as follows. Given s, x we iterateover (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weightsequal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , thenwe have identified a violated cycle inequality associated with s.We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in thesupplementary material. To accelerate optimization, we add MWR in addition to standard Bendersrows, which we describe in the following Sec. 5.Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,1provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗vi vj = 1, if xvi vj > 2∗and otherwise set x∗∗vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate∗∗connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of thefeasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C(supplementary material), we provide a more involved approach to produce feasible integer solutions.In this section, we characterized CC using Benders decomposition and provided a cutting planealgorithm to solve the corresponding optimization.5Magnanti-Wong Benders RowsWe accelerate Benders decomposition (see Sec. 4) using the classic operations research technique ofMagnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tightbound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,6Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for variousvalues of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,and black for not using Magnanti-Wong rows. We show both the computation time with and withoutexploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles toindicate the approximate difficulty of the problem as ranked by input file size of 100 files.Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot thetotal running time versus the total running time when solving each subproblem is done on its ownCPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR arenot used. We draw a line with slope=1 in magenta to better enable appreciation of the red and blackpoints. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when usingparallel processing, is the maximum time spent to solve any sub-problem for that iteration.while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),where we replace the objective and add one additional constraint.We follow the tradition of the operations research literature and use a random negative valued vector(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders−1subproblem is solved. We experimented with using as an objective .0001+|φ, which encouragesvi vj |the cutting of edges with large positive weight, but it works as well as the random negative objective.Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within atolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).XXX+τ Q(φ, s, x) ≤ −(λ−ψv− xvs v −ψv+ xvs vvi vj + λvi vj )xvi vj +(vs ,v)∈Es−(vi ,vj )∈(E + \Es+ )(vs ,v)∈Es+(9)Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In ourexperiments, we found that τ = 12 provides strong performance.6Experiments: Image SegmentationIn this section, we demonstrate the value of our algorithm BDCC on CC problem instances for imagesegmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experimentsdemonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically acceleratesoptimization.To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use therandom unit norm negative valued objective when generating MWR. We use CPLEX to solve alllinear and integer linear programming problems considered during the course of optimization. We usea maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).7Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that noMWR are generated.=0.1=1=10τpar10501003000.500.5000110.1490.01060.2660.04260.3720.05320.7770.07450.5850.07450.9040.07450.8940.1060.9680.138τpar10501003000.500.5000110.1490.01060.3190.05320.3940.06380.8190.07450.6060.07450.9470.1060.9040.160.9790.17τpar10501003000.500.5000110.2020.05320.4470.06380.4260.09570.9360.1280.6280.1280.9790.1810.9150.2230.9890.287We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally thatsolving for the minimum vertex cover consumed negligible CPU time for our dataset. We attributethis fact to the structure of our problem domain, since the minimum vertex cover is an NP-hardproblem. For problem instances where solving for the minimum vertex cover exactly is difficult, theminimum vertex cover problem can be solved approximately or greedily.In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problemdifficulties. We observe that the presence of MWR dramatically accelerates optimization. However,the exact value of τ does not effect the speed of optimization dramatically. We show performancewith and without relying on parallel processing. Our parallel processing times assume that wehave one CPU for each subproblem. For the problem instances in our application the number ofsubproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefitsof parallelization for all settings of τ . However, when MWR are not used, we observe diminishedimprovement, since the master problem consumes a larger proportion of total CPU time.In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most probleminstances, the total CPU time required when using no MWR was prohibitively large, which is not thecase when MWR are employed. Thus most problem instances solved without MWR terminated early.In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWRare generated). We consider a set of tolerances on convergence regarding the duality gap, which isthe difference between the anytime solution (upper bound) and the lower bound on the objective. Foreach such tolerance , we compute the percentage of instances, for which the duality gap is less than, after various amounts of time. We observe that the performance of optimization without MWR,but exploiting parallelization performs worse than using MWR, but without paralleliziation. Thisdemonstrates that, across the dataset, MWR are of greater importance than parallelization.7</corps> <conclusion>ConclusionsWe present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Ourmethod exploits the Benders decomposition to avoid the enumeration of a large number of cycleinequalities. This offers a new technique in the toolkit of linear programming relaxations, that weexpect will find further use in the application of combinatorial optimization to problems in computervision.8The exploitation of results from the domain of operations research may lead to improved variantsof BDCC. For example, one can intelligently select the subproblems to solve instead of solving allsubproblems in each iteration. This strategy is referred to as partial pricing in the operations researchliterature. Similarly one can devote a minimum amount of time in each iteration to solve the masterproblem so as to enforce integrality on a subset of the variables of the master problem.</conclusion><acknowledgement></acknowledgement> <references>References[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentationwith closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision(ICCV-11), pages 2611–2618, 2011.[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the TwelvethInternational Conference on Computer Vision (ECCV-12), 2012.[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmentingplanar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the NinthConference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages238–247, 2002.[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximatesolver for multicut partitioning. In CVPR, 2014.[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for theminimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:10.1007/978-3-319-46475-6_44.[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerischemathematik, 4(1):238–252, 1962.[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operationsresearch, 33(5):989–1007, 1985.[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraftrouting and crew scheduling. Transportation science, 35(4):375–388, 2001.[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):1776–1781, 1966.[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):399–404, 1956.[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.Management science, 20(5):822–844, 1974.[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. OperationsResearch (volume 9), 1961.[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.Springer, 2016.[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. InACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,2015.9[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition ofimage and mesh graphs by lifted multicuts. In ICCV, 2015.[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.Mathematical Programming Computation, 1(1):43–67, 2009.[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement andmodel selection criteria. Operations research, 29(3):464–484, 1981.[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and itsapplication to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings ofthe Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning andunsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,pages 769–776. ACM, 2009.[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlationclustering on big graphs. In Proceedings of the 28th International Conference on Neural InformationProcessing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URLhttp://dl.acm.org/citation.cfm?id=2969239.2969249.[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pages 4929–4937, 2016.[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roofduality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,june 2007.[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,IEEE Transactions on, 39(5):694–697, May 1990.[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. InCVPR, 2017.[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. InCVPR, 2015.[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation withbenders decomposition. arXiv preprint arXiv:1709.04411, 2017.[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference onComputer Vision (ECCV), pages 652–666, 2018.[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural InformationProcessing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information ProcessingSystems, 2015.[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXivpreprint arXiv:1805.04958, 2018.[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. InProceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright fieldmicroscopy. In ISBI, 2014.10AAPPENDIX: Q(φ, s, x∗ ) = 0 at OptimalityIn this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗vi vj )s∈S } is constructed, for whichQ(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms ofxs .Mx∗vi vj = xvi vj + max xsvi vjs∈S∀(vi , vj ) ∈ E +Mx∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ SM∀(vi , vj ) ∈ E +M∀(vi , vj ) ∈ Es− , s ∈ S.xs∗vi vj = 0xs∗vi vj = 1(10)The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to theoptimizing solution for f in subproblem s, given x, x∗ respectively.x∗vi vj = xvi vj + max fvsi vjs∈Sx∗vi vj = xvi vj − fvsi vj∀(vi , vj ) ∈ E +∀(vi , vj ) ∈ Es− , s ∈ Sfvs∗= 0 ∀(vi , vj ) ∈ E +i vj(11)fvs∗= 0 ∀(vi , vj ) ∈ Es−i vjThese updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, thatsince f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the totalPdecrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than theformer value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other handthe total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yieldsin an increase of the objective of the master problem by −φvi vj (1 − xnvi vj ), while the objective of subproblems decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .BLine by Line Description of BDCCWe provide the line by line description of Alg. 1.• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.• Line 2: Indicate that we have not solved the LP relaxation yet.• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasibleintegral solution is produced.1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycleinequalities. We enforce integrality if we have finished solving the LP relaxation, which isindicated by done_lp=True.2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is doneby iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less thanxvi vj . This distance is defined on the graph’s edges E with weights equal to x.– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascentset Ẑ.– Line 11: Indicate that a Benders row was added this iteration.4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, whensolving the master problem for the remainder of the algorithm.• Line 18 Return solution x.11CGenerating Feasible Integer Solutions Prior to ConvergencePrior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is sothat a practitioner can terminate optimization, when the gap between the objectives of the integral solution andthe relaxation is small. In this section we consider the production of feasible integer solutions, given the currentsolution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to thisprocedure as rounding.Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determinedusing x∗ below.κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +κvi vj =φvi vj x∗vi vj∀(vi , vj ) ∈ E(12)−∗Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Letxs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗vi vj = 1 if exactlyone of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, wheres∗x0sas the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).vi vj = 1Es− (vi , vj ), is achieved using xs∗The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasiblethen the solution produced below has cost equal to that of x∗ .Mxs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ SMs∗x+vi vj = max xvi vjs∈SMs∗x+vi vj = xvi vj∀(vi , vj ) ∈ E +(13)∀(vi , vj ) ∈ Es− , s ∈ SThe procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close tointegral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting ofedges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Inputx∗ )1: x+vi vj = 0 ∀(vi , vj ) ∈ E2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +4: for s ∈ S do5:xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.+s6:x+vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E+7:κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E8: end for9: Return x+• Line 1: Initialize x+ as the zero vector.• Line 2-3: Set κ according to Eq. (12)• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.2. Line 6: Cut edges in x+ that are cut in xs .3. Line 7: Set φvi vj to zero for cut edges in x+ .• Line 9: Return the solution x+When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],though we do not exploit its capacity to tackle non-submodular problems.12</references><discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.  easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2  constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3  4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.  1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4  We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5  Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6  Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7  Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated. =0.1  =1  =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance , we compute the percentage of instances, for which the duality gap is less than , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8  The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9  [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10  A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11  C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12  </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.  easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2  constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3  4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.  1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4  We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5  Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6  Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7  Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated. =0.1  =1  =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance , we compute the percentage of instances, for which the duality gap is less than , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8  The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9  [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10  A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11  C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12  </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.  easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2  constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3  4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.  1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4  We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5  Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6  Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7  Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated. =0.1  =1  =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance , we compute the percentage of instances, for which the duality gap is less than , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8  The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9  [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10  A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11  C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12  </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.  easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2  constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3  4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.  1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4  We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5  Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6  Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7  Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated. =0.1  =1  =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance , we compute the percentage of instances, for which the duality gap is less than , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8  The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9  [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10  A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11  C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12  </references> <discussion></discussion></informations> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj )  xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj  2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion> <titre></titre>  <auteur></auteur> <abstract>Abstract We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance’s graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.  </abstract> <introduction>Introduction  Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities. A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined on a sparse graph with real valued edge weights, where nodes correspond to observations and weighted edges describe the affinity between pairs of nodes. For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 . The magnitude of the weight is a function of the confidence of the classifier. The CC cost function sums up the weights of the edges separating connected components (referred to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges naturally as a function of the edge weights, rather than requiring an additional search over some model order parameter describing the number of clusters (entities) [37]. Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear programming with cutting planes. They do not scale easily to large CC problem instances and are not Preprint. Under review.   easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in CC for domains, where massively parallel computation could be employed. In this paper we apply the classic Benders decomposition from operations research [10] to CC for computer vision. Benders decomposition is commonly applied in operations research to solve mixed integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The block structure requires that no row of the constraint matrix of the MILP contains variables from more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem. Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization proceeds with the master problem solving optimization over its variables. The subsequent solution of the subproblems can be done in parallel and provides primal/dual solutions over their variables conditioned on the solution to the master problem. The dual solutions to the subproblems provide constraints to the master problem. Optimization continues until no further constraints are added to the master problem. Benders decomposition is an exact MILP programming solver, but can be intuitively understood as a coordinate descent procedure, iterating between the master problem and the subproblems. Here, solving the subproblems not only provides a solution for their variables, but also a lower bound in the form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current solution to the master problem. Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an alternative (often random) objective under the hard constraint of optimality (possibly within a factor) regarding the original objective of the subproblem. Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].  </introduction>  <corps>2  Related Work  Correlation clustering has been successfully applied to multiple problems in computer vision including image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation. The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies on random sampling and only provides optimality bounds. Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al. [39], Andres et al. [3]. Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the quality of the solution. Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes correspond to detections of objects and edges are associated with probabilities of co-association.The work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate multi-person pose estimation using CC augmented with node labeling. Our work is derived from the classical work in operations research on Benders decomposition [10, 11, 15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of fractional variables (flows of commodities from facilities to customers in a network) associated with 2   constraints. Benders decomposition reformulates optimization so as to use only the integer variables and converts the fractional variables into constraints. These constraints are referred to as Benders rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the use of MWR [23], which are more binding than the standard Benders rows. Benders decomposition has recently been introduced to computer vision (though not for CC), for the purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is modeled so as to admit efficient optimization, using column generation and Benders decomposition jointly. The application of Benders decomposition in our paper is distinct regarding the problem domain, the underlying integer program and the structure of the Benders subproblems.  3  Standard Correlation Clustering Formulation  In this section, we review the standard optimization formulation for CC [1], which corresponds to a graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following binary edge labeling problem. Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized: min  x∈{0,1}|E|  s.t.  X (vi ,vj )∈E −  X  X  −φvi vj (1 − xvi vj ) +  φ vi vj x vi vj  (CC1 )  (vi ,vj )∈E +  xvi vj ≥ xvic vjc  ∀c ∈ C,  (1)  (vi ,vj )∈Ec+  where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative, respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c. Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge (vi , vj ) with xvi vj = 1 as a cut edge. The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1) ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct components. We refer to the constraints in Eq. (1) as cycle inequalities. Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1] generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ (initialized empty) and adding new constraints from the set of currently violated cycle inequalities. Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the ˆ The corresponding path has total weight less than xvi vj , the corresponding constraint is added to C. LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated cycle inequalities exist, after which the ILP must be solved in each iteration. We should note that earlier work in CC for computer vision did not require that cycle inequalities contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E + on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1) or its LP relaxation. In this section, we reviewed the baseline approach for solving CC in the computer vision community. In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the specific solver of Andres et al. [1]. 3   4  Benders Decomposition for Correlation Clustering  In this section, we introduce a novel approach to CC using Benders decomposition (referred to as BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems, such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+ to denote the subset of E + adjacent to vs . In this section, we assume that we are provided with S, which can be produced greedily or using an LP/ILP solver. Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the number of edges in the subproblem s. X X X (CC1 ) (CC2 ) : min −φvi vj (1 − xvi vj ) + φ vi vj x vi vj + Q(φ, s, x), x∈{0,1}|E|  (vi ,vj )∈E −  (vi ,vj )∈E +  s∈S  (CC2 ) where Q(φ, s, x) is defined as follows. Q(φ, s, x)  =  min s  x ∈{0,1}  s.t.  X |s|  −φvi vj (1 − xsvi vj ) +  (vi ,vj )∈Es−  X  X  φvi vj xsvi vj  (2)  (vi ,vj )∈E +  xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .  (vi ,vj )∈Ec+  We now construct a solution x∗ = {x∗vi vj , (xs∗ vi vj )s∈S } for which Eq. (CC2 ) is minimized and all cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed as follows. M  x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E + .  (3) (4)  The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2). Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗ vi vj and is defined as follows.   1, if (vi , vj ) ∈ Es− xs∗ = (5) vi vj 0, otherwise. In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗ vi vj )s∈S } is no greater than that of {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all s ∈ S. It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for all s ∈ S. Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is 2-colorable. This is because any partition xs can be altered without increasing its cost, by merging connected components that are adjacent to one another, not including the root node vs . Note, that merging any pair of such components, does not increase the cost, since those components are not separated by negative weight edges in subproblem s and so the result is still a partition. Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node labeling formulation of min-cut, with the notation below. 4   We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let ( 1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x s f vi vj = (6) 1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x. Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all (vi , vj ) ∈ Es− . Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of the optimization being totally unimodular, given that x is binary. Total unimodularity is a known property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following. X X Q(φ, s, x) = smin φvi vj fvsi vj − φvs v fvss v (7) fv v ≥0 i j (vi ,vj )∈E + mv ≥0  (vs ,v)∈Es−  λ− vi vj  :  mvi − mvj ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  λ+ vi vj  :  mvj − mvi ≤ xvi vj + fvsi vj  ∀(vi , vj ) ∈ (E + \ Es+ ),  ψv−  :  xvs v − fvss v ≤ mv  ∀(vs , v) ∈ Es− ,  ψv+  :  mv ≤ xvs v + fvss v  ∀(vs , v) ∈ Es+ ,  This yields to the corresponding dual subproblem. X + max − (λ− vi vj + λvi vj )xvi vj + λ≥0 ψ≥0  s.t.  ψv+i  X  ψv− xvs v −  (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  X  ψv+ xvs v  (8)  (vs ,v)∈Es+  1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+ X  X  + (λ− vi vj − λvi vj ) +  vj (vi ,vj )∈(E + \Es+ )  φ vi vj  − (λ+ vj vi − λvj vi ) ≥ 0  ∀vi ∈ V − vs  vj (vj ,vi )∈(E + \Es+ )  −φvs v − ψv− ≥ 0 φvs v − ψv+ ≥ 0 + − (λ− vi v j + λ vi vj ) ≥ 0  ∀(vs , v) ∈ Es− ∀(vs , v) ∈ Es+ ∀(vi , vj ) ∈ (E + \ Es+ ).  In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the xvi vj term.  + −(λ− if (vi , vj ) ∈ (E + \ Es+ )  vi vj + λvi vj ),     −ψv+j , if (vi , vj ) ∈ Es+ ωvzi vj =  ψv−j , if (vi , vj ) ∈ Es−     0, if (vi , vj ) ∈ (E − \ Es− ). We denote the set of all dual feasible solutions across s P ∈ S as Z, with z ∈ Z. Observe, that to enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z. We formulate CC as optimization using Z below. X X (CC2 ) (CC3 ) = min φ vi vj x vi vj − (1 − xvi vj )φvi vj (CC3 ) x∈{0,1}|E|  s.t.  X  (vi ,vj )∈E −  (vi ,vj )∈E +  xvi vj ωvzi vj ≤ 0  ∀z ∈ Z  (vi ,vj )∈E  5   Algorithm 1 Benders Decomposition for CC (BDCC) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18:  4.1  Ẑ = {} done_LP = False repeat x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True did_add = False for s ∈ S do if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then z1 = Get Benders row via Eq (8). z2 = Get MWR via Sec. 5. Ẑ = Ẑ ∪ z1 ∪ z2 did_add = True end if end for if did_add=False then done_LP = True end if until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E Return x  Cutting Plane Optimization  Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the master problem) and generating new Benders rows until no violated constraints exist. This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ. By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver. To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then we have identified a violated cycle inequality associated with s. We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the supplementary material. To accelerate optimization, we add MWR in addition to standard Benders rows, which we describe in the following Sec. 5. Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x, 1 provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗ vi vj = 1, if xvi vj > 2 ∗ and otherwise set x∗∗ vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate ∗∗ connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C (supplementary material), we provide a more involved approach to produce feasible integer solutions. In this section, we characterized CC using Benders decomposition and provided a cutting plane algorithm to solve the corresponding optimization.  5  Magnanti-Wong Benders Rows  We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However, ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ , 6   Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively, and black for not using Magnanti-Wong rows. We show both the computation time with and without exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to indicate the approximate difficulty of the problem as ranked by input file size of 100 files. Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the total running time versus the total running time when solving each subproblem is done on its own CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using parallel processing, is the maximum time spent to solve any sub-problem for that iteration. while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8), where we replace the objective and add one additional constraint. We follow the tradition of the operations research literature and use a random negative valued vector (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders −1 subproblem is solved. We experimented with using as an objective .0001+|φ , which encourages vi vj | the cutting of edges with large positive weight, but it works as well as the random negative objective. Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite. Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8). X X X + τ Q(φ, s, x) ≤ − (λ− ψv− xvs v − ψv+ xvs v vi vj + λvi vj )xvi vj + (vs ,v)∈Es−  (vi ,vj )∈(E + \Es+ )  (vs ,v)∈Es+  (9) Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our experiments, we found that τ = 12 provides strong performance.  6  Experiments: Image Segmentation  In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates optimization. To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS. This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the random unit norm negative valued objective when generating MWR. We use CPLEX to solve all linear and integer linear programming problems considered during the course of optimization. We use a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization). 7   Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance  , within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization. We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no MWR are generated.  =0.1   =1   =10  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.266 0.0426  0.372 0.0532 0.777 0.0745  0.585 0.0745 0.904 0.0745  0.894 0.106 0.968 0.138  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.149 0.0106 0.319 0.0532  0.394 0.0638 0.819 0.0745  0.606 0.0745 0.947 0.106  0.904 0.16 0.979 0.17  τ  par  10  50  100  300  0.5 0 0.5 0  0 0 1 1  0.202 0.0532 0.447 0.0638  0.426 0.0957 0.936 0.128  0.628 0.128 0.979 0.181  0.915 0.223 0.989 0.287  We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈ E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S, we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the minimum vertex cover problem can be solved approximately or greedily. In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem difficulties. We observe that the presence of MWR dramatically accelerates optimization. However, the exact value of τ does not effect the speed of optimization dramatically. We show performance with and without relying on parallel processing. Our parallel processing times assume that we have one CPU for each subproblem. For the problem instances in our application the number of subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits of parallelization for all settings of τ . However, when MWR are not used, we observe diminished improvement, since the master problem consumes a larger proportion of total CPU time. In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem instances, the total CPU time required when using no MWR was prohibitively large, which is not the case when MWR are employed. Thus most problem instances solved without MWR terminated early. In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR are generated). We consider a set of tolerances on convergence regarding the duality gap, which is the difference between the anytime solution (upper bound) and the lower bound on the objective. For each such tolerance  , we compute the percentage of instances, for which the duality gap is less than  , after various amounts of time. We observe that the performance of optimization without MWR, but exploiting parallelization performs worse than using MWR, but without paralleliziation. This demonstrates that, across the dataset, MWR are of greater importance than parallelization.  7  </corps>  <conclusion>Conclusions  We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our method exploits the Benders decomposition to avoid the enumeration of a large number of cycle inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we expect will find further use in the application of combinatorial optimization to problems in computer vision. 8   The exploitation of results from the domain of operations research may lead to improved variants of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all subproblems in each iteration. This strategy is referred to as partial pricing in the operations research literature. Similarly one can devote a minimum amount of time in each iteration to solve the master problem so as to enforce integrality on a subset of the variables of the master problem.  </conclusion> <acknowledgement></acknowledgement>  <references>References [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision (ICCV-11), pages 2611–2618, 2011. [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth International Conference on Computer Vision (ECCV-12), 2012. [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013. [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014. [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages 238–247, 2002. [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996. [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate solver for multicut partitioning. In CVPR, 2014. [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015. [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi: 10.1007/978-3-319-46475-6_44. [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische mathematik, 4(1):238–252, 1962. [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research, 33(5):989–1007, 1985. [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft routing and crew scheduling. Transportation science, 35(4):375–388, 2001. [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10): 1776–1781, 1966. [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3): 399–404, 1956. [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition. Management science, 20(5):822–844, 1974. [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations Research (volume 9), 1961. [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016. [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018. [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV, 2015.  9   [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In ICCV, 2015. [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011. [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):43–67, 2009. [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and model selection criteria. Operations research, 29(3):464–484, 1981. [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001. [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769–776. ACM, 2009. [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969239.2969249. [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, june 2007. [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers, IEEE Transactions on, 39(5):694–697, May 1990. [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In CVPR, 2017. [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In CVPR, 2015. [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with benders decomposition. arXiv preprint arXiv:1709.04411, 2017. [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 652–666, 2018. [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015. [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing Systems, 2015. [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv preprint arXiv:1805.04958, 2018. [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012. [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition. In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014. [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field microscopy. In ISBI, 2014.  10   A  APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality  In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0. Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗ vi vj )s∈S } is constructed, for which Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of xs . M  x∗vi vj = xvi vj + max xsvi vj s∈S  ∀(vi , vj ) ∈ E +  M  x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S M  ∀(vi , vj ) ∈ E +  M  ∀(vi , vj ) ∈ Es− , s ∈ S.  xs∗ vi vj = 0 xs∗ vi vj = 1  (10)  The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the optimizing solution for f in subproblem s, given x, x∗ respectively. x∗vi vj = xvi vj + max fvsi vj s∈S  x∗vi vj = xvi vj − fvsi vj  ∀(vi , vj ) ∈ E +  ∀(vi , vj ) ∈ Es− , s ∈ S  fvs∗ = 0 ∀(vi , vj ) ∈ E + i vj  (11)  fvs∗ = 0 ∀(vi , vj ) ∈ Es− i vj These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S. We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10), which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total P decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields in an increase of the objective of the master problem by −φvi vj (1 − xn vi vj ), while the objective of subproblem s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .  B  Line by Line Description of BDCC  We provide the line by line description of Alg. 1. • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set. • Line 2: Indicate that we have not solved the LP relaxation yet. • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible integral solution is produced. 1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle inequalities. We enforce integrality if we have finished solving the LP relaxation, which is indicated by done_lp=True. 2. Line 5: Indicate that we have not yet added any Benders rows to this iteration. 3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities. – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than xvi vj . This distance is defined on the graph’s edges E with weights equal to x. – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent set Ẑ. – Line 11: Indicate that a Benders row was added this iteration. 4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when solving the master problem for the remainder of the algorithm. • Line 18 Return solution x.  11   C  Generating Feasible Integer Solutions Prior to Convergence  Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and the relaxation is small. In this section we consider the production of feasible integer solutions, given the current solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this procedure as rounding. Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined using x∗ below. κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + κvi vj =  φvi vj x∗vi vj  ∀(vi , vj ) ∈ E  (12)  −  ∗  Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗ vi vj = 1 if exactly one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where s∗ x0s as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7). vi vj = 1Es− (vi , vj ), is achieved using x s∗ The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible then the solution produced below has cost equal to that of x∗ . M  xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S M  s∗ x+ vi vj = max xvi vj s∈S  M  s∗ x+ vi vj = xvi vj  ∀(vi , vj ) ∈ E +  (13)  ∀(vi , vj ) ∈ Es− , s ∈ S  The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ. We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+ by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.  Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input x∗ ) 1: x+ vi vj = 0 ∀(vi , vj ) ∈ E 2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E − 3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E + 4: for s ∈ S do 5: xs = minimizer for Q(κ, s, x0s ) given fixed κ, s. + s 6: x+ vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E + 7: κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E 8: end for 9: Return x+ • Line 1: Initialize x+ as the zero vector. • Line 2-3: Set κ according to Eq. (12) • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem. 1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s. 2. Line 6: Cut edges in x+ that are cut in xs . 3. Line 7: Set φvi vj to zero for cut edges in x+ . • Line 9: Return the solution x+ When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28], though we do not exploit its capacity to tackle non-submodular problems.  12   </references> <discussion></discussion>
<titre></titre> 
<auteur></auteur>
<abstract>Abstract
We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We
reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders
decomposition formulation has many subproblems, each associated with a node in
the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
enforces the cycle inequalities corresponding to edges with negative (repulsive)
weights attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
approaches, theoretically allows for massive parallelization.

</abstract>
<introduction>Introduction

Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
on a sparse graph with real valued edge weights, where nodes correspond to observations and
weighted edges describe the affinity between pairs of nodes.
For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .
The magnitude of the weight is a function of the confidence of the classifier.
The CC cost function sums up the weights of the edges separating connected components (referred
to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
naturally as a function of the edge weights, rather than requiring an additional search over some
model order parameter describing the number of clusters (entities) [37].
Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
programming with cutting planes. They do not scale easily to large CC problem instances and are not
Preprint. Under review.

easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
CC for domains, where massively parallel computation could be employed.
In this paper we apply the classic Benders decomposition from operations research [10] to CC for
computer vision. Benders decomposition is commonly applied in operations research to solve mixed
integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The
block structure requires that no row of the constraint matrix of the MILP contains variables from
more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
proceeds with the master problem solving optimization over its variables. The subsequent solution
of the subproblems can be done in parallel and provides primal/dual solutions over their variables
conditioned on the solution to the master problem. The dual solutions to the subproblems provide
constraints to the master problem. Optimization continues until no further constraints are added to
the master problem.
Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
solving the subproblems not only provides a solution for their variables, but also a lower bound in the
form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
solution to the master problem.
Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
regarding the original objective of the subproblem.
Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].

</introduction> 
<corps>2

Related Work

Correlation clustering has been successfully applied to multiple problems in computer vision including
image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
on random sampling and only provides optimality bounds.
Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
[39], Andres et al. [3].
Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
quality of the solution.
Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
correspond to detections of objects and edges are associated with probabilities of co-association.The
work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
multi-person pose estimation using CC augmented with node labeling.
Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
fractional variables (flows of commodities from facilities to customers in a network) associated with
2

constraints. Benders decomposition reformulates optimization so as to use only the integer variables
and converts the fractional variables into constraints. These constraints are referred to as Benders
rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
use of MWR [23], which are more binding than the standard Benders rows.
Benders decomposition has recently been introduced to computer vision (though not for CC), for the
purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
modeled so as to admit efficient optimization, using column generation and Benders decomposition
jointly. The application of Benders decomposition in our paper is distinct regarding the problem
domain, the underlying integer program and the structure of the Benders subproblems.

3

Standard Correlation Clustering Formulation

In this section, we review the standard optimization formulation for CC [1], which corresponds to a
graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
binary edge labeling problem.
Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
min

x∈{0,1}|E|

s.t.

X
(vi ,vj )∈E −

X

X

−φvi vj (1 − xvi vj ) +

φ vi vj x vi vj

(CC1 )

(vi ,vj )∈E +

xvi vj ≥ xvic vjc

∀c ∈ C,

(1)

(vi ,vj )∈Ec+

where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
(vi , vj ) with xvi vj = 1 as a cut edge.
The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
components. We refer to the constraints in Eq. (1) as cycle inequalities.
Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
ˆ The
corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
cycle inequalities exist, after which the ILP must be solved in each iteration.
We should note that earlier work in CC for computer vision did not require that cycle inequalities
contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
or its LP relaxation.
In this section, we reviewed the baseline approach for solving CC in the computer vision community.
In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
specific solver of Andres et al. [1].
3

4

Benders Decomposition for Correlation Clustering

In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
to denote the subset of E + adjacent to vs .
In this section, we assume that we are provided with S, which can be produced greedily or using an
LP/ILP solver.
Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
number of edges in the subproblem s.
X
X
X
(CC1 )
(CC2 ) :
min
−φvi vj (1 − xvi vj ) +
φ vi vj x vi vj +
Q(φ, s, x),
x∈{0,1}|E|

(vi ,vj )∈E −

(vi ,vj )∈E +

s∈S

(CC2 )
where Q(φ, s, x) is defined as follows.
Q(φ, s, x)

=

min
s

x ∈{0,1}

s.t.

X
|s|

−φvi vj (1 − xsvi vj ) +

(vi ,vj )∈Es−

X

X

φvi vj xsvi vj

(2)

(vi ,vj )∈E +

xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

(vi ,vj )∈Ec+

We now construct a solution x∗ = {x∗vi vj , (xs∗
vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
as follows.
M

x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E + .

(3)
(4)

The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
vi vj and
is defined as follows.

1, if (vi , vj ) ∈ Es−
xs∗
=
(5)
vi vj
0, otherwise.
In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
vi vj )s∈S } is no greater than that of
{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all
s ∈ S.
It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
all s ∈ S.
Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
connected components that are adjacent to one another, not including the root node vs . Note, that
merging any pair of such components, does not increase the cost, since those components are not
separated by negative weight edges in subproblem s and so the result is still a partition.
Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
labeling formulation of min-cut, with the notation below.
4

We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
(
1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
s
f vi vj =
(6)
1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all
(vi , vj ) ∈ Es− .
Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
X
X
Q(φ, s, x) = smin
φvi vj fvsi vj −
φvs v fvss v
(7)
fv v ≥0
i j
(vi ,vj )∈E +
mv ≥0

(vs ,v)∈Es−

λ−
vi vj

:

mvi − mvj ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

λ+
vi vj

:

mvj − mvi ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

ψv−

:

xvs v − fvss v ≤ mv

∀(vs , v) ∈ Es− ,

ψv+

:

mv ≤ xvs v + fvss v

∀(vs , v) ∈ Es+ ,

This yields to the corresponding dual subproblem.
X
+
max −
(λ−
vi vj + λvi vj )xvi vj +
λ≥0
ψ≥0

s.t.

ψv+i

X

ψv− xvs v −

(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

X

ψv+ xvs v

(8)

(vs ,v)∈Es+

1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
X

X

+
(λ−
vi vj − λvi vj ) +

vj
(vi ,vj )∈(E + \Es+ )

φ vi vj

−
(λ+
vj vi − λvj vi ) ≥ 0

∀vi ∈ V − vs

vj
(vj ,vi )∈(E + \Es+ )

−φvs v − ψv− ≥ 0
φvs v − ψv+ ≥ 0
+
− (λ−
vi v j + λ vi vj ) ≥ 0

∀(vs , v) ∈ Es−
∀(vs , v) ∈ Es+
∀(vi , vj ) ∈ (E + \ Es+ ).

In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
xvi vj term.

+
−(λ−
if (vi , vj ) ∈ (E + \ Es+ )

vi vj + λvi vj ),




−ψv+j ,
if (vi , vj ) ∈ Es+
ωvzi vj =

ψv−j ,
if (vi , vj ) ∈ Es−




0,
if (vi , vj ) ∈ (E − \ Es− ).
We denote the set of all dual feasible solutions across s P
∈ S as Z, with z ∈ Z. Observe, that to
enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
We formulate CC as optimization using Z below.
X
X
(CC2 )
(CC3 ) = min
φ vi vj x vi vj −
(1 − xvi vj )φvi vj
(CC3 )
x∈{0,1}|E|

s.t.

X

(vi ,vj )∈E −

(vi ,vj )∈E +

xvi vj ωvzi vj ≤ 0

∀z ∈ Z

(vi ,vj )∈E

5

Algorithm 1 Benders Decomposition for CC (BDCC)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

4.1

Ẑ = {}
done_LP = False
repeat
x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
did_add = False
for s ∈ S do
if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then
z1 = Get Benders row via Eq (8).
z2 = Get MWR via Sec. 5.
Ẑ = Ẑ ∪ z1 ∪ z2
did_add = True
end if
end for
if did_add=False then
done_LP = True
end if
until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
Return x

Cutting Plane Optimization

Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
master problem) and generating new Benders rows until no violated constraints exist.
This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then
we have identified a violated cycle inequality associated with s.
We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
rows, which we describe in the following Sec. 5.
Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
1
provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
vi vj = 1, if xvi vj > 2
∗
and otherwise set x∗∗
vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
∗∗
connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
(supplementary material), we provide a more involved approach to produce feasible integer solutions.
In this section, we characterized CC using Benders decomposition and provided a cutting plane
algorithm to solve the corresponding optimization.

5

Magnanti-Wong Benders Rows

We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
6

Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
and black for not using Magnanti-Wong rows. We show both the computation time with and without
exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
total running time versus the total running time when solving each subproblem is done on its own
CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
where we replace the objective and add one additional constraint.
We follow the tradition of the operations research literature and use a random negative valued vector
(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
−1
subproblem is solved. We experimented with using as an objective .0001+|φ
, which encourages
vi vj |
the cutting of edges with large positive weight, but it works as well as the random negative objective.
Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
X
X
X
+
τ Q(φ, s, x) ≤ −
(λ−
ψv− xvs v −
ψv+ xvs v
vi vj + λvi vj )xvi vj +
(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

(vs ,v)∈Es+

(9)
Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
experiments, we found that τ = 12 provides strong performance.

6

Experiments: Image Segmentation

In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates
optimization.
To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
linear and integer linear programming problems considered during the course of optimization. We use
a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
7

Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
MWR are generated.
=0.1

=1

=10

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.266
0.0426

0.372
0.0532
0.777
0.0745

0.585
0.0745
0.904
0.0745

0.894
0.106
0.968
0.138

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.319
0.0532

0.394
0.0638
0.819
0.0745

0.606
0.0745
0.947
0.106

0.904
0.16
0.979
0.17

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.202
0.0532
0.447
0.0638

0.426
0.0957
0.936
0.128

0.628
0.128
0.979
0.181

0.915
0.223
0.989
0.287

We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
minimum vertex cover problem can be solved approximately or greedily.
In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
the exact value of τ does not effect the speed of optimization dramatically. We show performance
with and without relying on parallel processing. Our parallel processing times assume that we
have one CPU for each subproblem. For the problem instances in our application the number of
subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits
of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
improvement, since the master problem consumes a larger proportion of total CPU time.
In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
instances, the total CPU time required when using no MWR was prohibitively large, which is not the
case when MWR are employed. Thus most problem instances solved without MWR terminated early.
In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
the difference between the anytime solution (upper bound) and the lower bound on the objective. For
each such tolerance , we compute the percentage of instances, for which the duality gap is less than
, after various amounts of time. We observe that the performance of optimization without MWR,
but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
demonstrates that, across the dataset, MWR are of greater importance than parallelization.

7

</corps> 
<conclusion>Conclusions

We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
expect will find further use in the application of combinatorial optimization to problems in computer
vision.
8

The exploitation of results from the domain of operations research may lead to improved variants
of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
problem so as to enforce integrality on a subset of the variables of the master problem.

</conclusion>
<acknowledgement></acknowledgement> 
<references>References
[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
(ICCV-11), pages 2611–2618, 2011.
[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
International Conference on Computer Vision (ECCV-12), 2012.
[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting
planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
238–247, 2002.
[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
solver for multicut partitioning. In CVPR, 2014.
[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
10.1007/978-3-319-46475-6_44.
[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.
[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations
research, 33(5):989–1007, 1985.
[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft
routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
1776–1781, 1966.
[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
399–404, 1956.
[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
Management science, 20(5):822–844, 1974.
[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
Research (volume 9), 1961.
[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
Springer, 2016.
[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
2015.

9

[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
image and mesh graphs by lifted multicuts. In ICCV, 2015.
[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.
In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
Mathematical Programming Computation, 1(1):43–67, 2009.
[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
model selection criteria. Operations research, 29(3):464–484, 1981.
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 769–776. ACM, 2009.
[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2969239.2969249.
[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:
Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
june 2007.
[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
IEEE Transactions on, 39(5):694–697, May 1990.
[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
CVPR, 2017.
[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
CVPR, 2015.
[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 652–666, 2018.
[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
Systems, 2015.
[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
preprint arXiv:1805.04958, 2018.
[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.
In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
microscopy. In ISBI, 2014.

10

A

APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.
Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
vi vj )s∈S } is constructed, for which
Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of
xs .
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E +

M

x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
M

∀(vi , vj ) ∈ E +

M

∀(vi , vj ) ∈ Es− , s ∈ S.

xs∗
vi vj = 0
xs∗
vi vj = 1

(10)

The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the
optimizing solution for f in subproblem s, given x, x∗ respectively.
x∗vi vj = xvi vj + max fvsi vj
s∈S

x∗vi vj = xvi vj − fvsi vj

∀(vi , vj ) ∈ E +

∀(vi , vj ) ∈ Es− , s ∈ S

fvs∗
= 0 ∀(vi , vj ) ∈ E +
i vj

(11)

fvs∗
= 0 ∀(vi , vj ) ∈ Es−
i vj
These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that
since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),
which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total
P
decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the
former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand
the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields
in an increase of the objective of the master problem by −φvi vj (1 − xn
vi vj ), while the objective of subproblem
s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

B

Line by Line Description of BDCC

We provide the line by line description of Alg. 1.
• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
• Line 2: Indicate that we have not solved the LP relaxation yet.
• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
integral solution is produced.
1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
indicated by done_lp=True.
2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.
– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
set Ẑ.
– Line 11: Indicate that a Benders row was added this iteration.
4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
solving the master problem for the remainder of the algorithm.
• Line 18 Return solution x.

11

C

Generating Feasible Integer Solutions Prior to Convergence

Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so
that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and
the relaxation is small. In this section we consider the production of feasible integer solutions, given the current
solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this
procedure as rounding.
Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
using x∗ below.
κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
κvi vj =

φvi vj x∗vi vj

∀(vi , vj ) ∈ E

(12)

−

∗

Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let
xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
vi vj = 1 if exactly
one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where
s∗
x0s
as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
vi vj = 1Es− (vi , vj ), is achieved using x
s∗
The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
then the solution produced below has cost equal to that of x∗ .
M

xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
M

s∗
x+
vi vj = max xvi vj
s∈S

M

s∗
x+
vi vj = xvi vj

∀(vi , vj ) ∈ E +

(13)

∀(vi , vj ) ∈ Es− , s ∈ S

The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to
integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.
We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+
by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of
edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
x∗ )
1: x+
vi vj = 0 ∀(vi , vj ) ∈ E
2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
4: for s ∈ S do
5:
xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
+
s
6:
x+
vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
+
7:
κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
8: end for
9: Return x+
• Line 1: Initialize x+ as the zero vector.
• Line 2-3: Set κ according to Eq. (12)
• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
2. Line 6: Cut edges in x+ that are cut in xs .
3. Line 7: Set φvi vj to zero for cut edges in x+ .
• Line 9: Return the solution x+
When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],
though we do not exploit its capacity to tackle non-submodular problems.

12

</references>
<discussion></discussion>
<titre></titre> 
<auteur></auteur>
<abstract>Abstract
We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We
reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders
decomposition formulation has many subproblems, each associated with a node in
the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
enforces the cycle inequalities corresponding to edges with negative (repulsive)
weights attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
approaches, theoretically allows for massive parallelization.

</abstract>
<introduction>Introduction

Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
on a sparse graph with real valued edge weights, where nodes correspond to observations and
weighted edges describe the affinity between pairs of nodes.
For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .
The magnitude of the weight is a function of the confidence of the classifier.
The CC cost function sums up the weights of the edges separating connected components (referred
to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
naturally as a function of the edge weights, rather than requiring an additional search over some
model order parameter describing the number of clusters (entities) [37].
Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
programming with cutting planes. They do not scale easily to large CC problem instances and are not
Preprint. Under review.

easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
CC for domains, where massively parallel computation could be employed.
In this paper we apply the classic Benders decomposition from operations research [10] to CC for
computer vision. Benders decomposition is commonly applied in operations research to solve mixed
integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The
block structure requires that no row of the constraint matrix of the MILP contains variables from
more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
proceeds with the master problem solving optimization over its variables. The subsequent solution
of the subproblems can be done in parallel and provides primal/dual solutions over their variables
conditioned on the solution to the master problem. The dual solutions to the subproblems provide
constraints to the master problem. Optimization continues until no further constraints are added to
the master problem.
Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
solving the subproblems not only provides a solution for their variables, but also a lower bound in the
form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
solution to the master problem.
Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
regarding the original objective of the subproblem.
Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].

</introduction> 
<corps>2

Related Work

Correlation clustering has been successfully applied to multiple problems in computer vision including
image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
on random sampling and only provides optimality bounds.
Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
[39], Andres et al. [3].
Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
quality of the solution.
Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
correspond to detections of objects and edges are associated with probabilities of co-association.The
work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
multi-person pose estimation using CC augmented with node labeling.
Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
fractional variables (flows of commodities from facilities to customers in a network) associated with
2

constraints. Benders decomposition reformulates optimization so as to use only the integer variables
and converts the fractional variables into constraints. These constraints are referred to as Benders
rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
use of MWR [23], which are more binding than the standard Benders rows.
Benders decomposition has recently been introduced to computer vision (though not for CC), for the
purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
modeled so as to admit efficient optimization, using column generation and Benders decomposition
jointly. The application of Benders decomposition in our paper is distinct regarding the problem
domain, the underlying integer program and the structure of the Benders subproblems.

3

Standard Correlation Clustering Formulation

In this section, we review the standard optimization formulation for CC [1], which corresponds to a
graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
binary edge labeling problem.
Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
min

x∈{0,1}|E|

s.t.

X
(vi ,vj )∈E −

X

X

−φvi vj (1 − xvi vj ) +

φ vi vj x vi vj

(CC1 )

(vi ,vj )∈E +

xvi vj ≥ xvic vjc

∀c ∈ C,

(1)

(vi ,vj )∈Ec+

where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
(vi , vj ) with xvi vj = 1 as a cut edge.
The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
components. We refer to the constraints in Eq. (1) as cycle inequalities.
Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
ˆ The
corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
cycle inequalities exist, after which the ILP must be solved in each iteration.
We should note that earlier work in CC for computer vision did not require that cycle inequalities
contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
or its LP relaxation.
In this section, we reviewed the baseline approach for solving CC in the computer vision community.
In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
specific solver of Andres et al. [1].
3

4

Benders Decomposition for Correlation Clustering

In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
to denote the subset of E + adjacent to vs .
In this section, we assume that we are provided with S, which can be produced greedily or using an
LP/ILP solver.
Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
number of edges in the subproblem s.
X
X
X
(CC1 )
(CC2 ) :
min
−φvi vj (1 − xvi vj ) +
φ vi vj x vi vj +
Q(φ, s, x),
x∈{0,1}|E|

(vi ,vj )∈E −

(vi ,vj )∈E +

s∈S

(CC2 )
where Q(φ, s, x) is defined as follows.
Q(φ, s, x)

=

min
s

x ∈{0,1}

s.t.

X
|s|

−φvi vj (1 − xsvi vj ) +

(vi ,vj )∈Es−

X

X

φvi vj xsvi vj

(2)

(vi ,vj )∈E +

xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

(vi ,vj )∈Ec+

We now construct a solution x∗ = {x∗vi vj , (xs∗
vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
as follows.
M

x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E + .

(3)
(4)

The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
vi vj and
is defined as follows.

1, if (vi , vj ) ∈ Es−
xs∗
=
(5)
vi vj
0, otherwise.
In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
vi vj )s∈S } is no greater than that of
{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all
s ∈ S.
It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
all s ∈ S.
Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
connected components that are adjacent to one another, not including the root node vs . Note, that
merging any pair of such components, does not increase the cost, since those components are not
separated by negative weight edges in subproblem s and so the result is still a partition.
Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
labeling formulation of min-cut, with the notation below.
4

We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
(
1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
s
f vi vj =
(6)
1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all
(vi , vj ) ∈ Es− .
Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
X
X
Q(φ, s, x) = smin
φvi vj fvsi vj −
φvs v fvss v
(7)
fv v ≥0
i j
(vi ,vj )∈E +
mv ≥0

(vs ,v)∈Es−

λ−
vi vj

:

mvi − mvj ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

λ+
vi vj

:

mvj − mvi ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

ψv−

:

xvs v − fvss v ≤ mv

∀(vs , v) ∈ Es− ,

ψv+

:

mv ≤ xvs v + fvss v

∀(vs , v) ∈ Es+ ,

This yields to the corresponding dual subproblem.
X
+
max −
(λ−
vi vj + λvi vj )xvi vj +
λ≥0
ψ≥0

s.t.

ψv+i

X

ψv− xvs v −

(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

X

ψv+ xvs v

(8)

(vs ,v)∈Es+

1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
X

X

+
(λ−
vi vj − λvi vj ) +

vj
(vi ,vj )∈(E + \Es+ )

φ vi vj

−
(λ+
vj vi − λvj vi ) ≥ 0

∀vi ∈ V − vs

vj
(vj ,vi )∈(E + \Es+ )

−φvs v − ψv− ≥ 0
φvs v − ψv+ ≥ 0
+
− (λ−
vi v j + λ vi vj ) ≥ 0

∀(vs , v) ∈ Es−
∀(vs , v) ∈ Es+
∀(vi , vj ) ∈ (E + \ Es+ ).

In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
xvi vj term.

+
−(λ−
if (vi , vj ) ∈ (E + \ Es+ )

vi vj + λvi vj ),




−ψv+j ,
if (vi , vj ) ∈ Es+
ωvzi vj =

ψv−j ,
if (vi , vj ) ∈ Es−




0,
if (vi , vj ) ∈ (E − \ Es− ).
We denote the set of all dual feasible solutions across s P
∈ S as Z, with z ∈ Z. Observe, that to
enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
We formulate CC as optimization using Z below.
X
X
(CC2 )
(CC3 ) = min
φ vi vj x vi vj −
(1 − xvi vj )φvi vj
(CC3 )
x∈{0,1}|E|

s.t.

X

(vi ,vj )∈E −

(vi ,vj )∈E +

xvi vj ωvzi vj ≤ 0

∀z ∈ Z

(vi ,vj )∈E

5

Algorithm 1 Benders Decomposition for CC (BDCC)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

4.1

Ẑ = {}
done_LP = False
repeat
x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
did_add = False
for s ∈ S do
if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then
z1 = Get Benders row via Eq (8).
z2 = Get MWR via Sec. 5.
Ẑ = Ẑ ∪ z1 ∪ z2
did_add = True
end if
end for
if did_add=False then
done_LP = True
end if
until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
Return x

Cutting Plane Optimization

Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
master problem) and generating new Benders rows until no violated constraints exist.
This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then
we have identified a violated cycle inequality associated with s.
We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
rows, which we describe in the following Sec. 5.
Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
1
provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
vi vj = 1, if xvi vj > 2
∗
and otherwise set x∗∗
vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
∗∗
connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
(supplementary material), we provide a more involved approach to produce feasible integer solutions.
In this section, we characterized CC using Benders decomposition and provided a cutting plane
algorithm to solve the corresponding optimization.

5

Magnanti-Wong Benders Rows

We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
6

Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
and black for not using Magnanti-Wong rows. We show both the computation time with and without
exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
total running time versus the total running time when solving each subproblem is done on its own
CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
where we replace the objective and add one additional constraint.
We follow the tradition of the operations research literature and use a random negative valued vector
(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
−1
subproblem is solved. We experimented with using as an objective .0001+|φ
, which encourages
vi vj |
the cutting of edges with large positive weight, but it works as well as the random negative objective.
Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
X
X
X
+
τ Q(φ, s, x) ≤ −
(λ−
ψv− xvs v −
ψv+ xvs v
vi vj + λvi vj )xvi vj +
(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

(vs ,v)∈Es+

(9)
Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
experiments, we found that τ = 12 provides strong performance.

6

Experiments: Image Segmentation

In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates
optimization.
To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
linear and integer linear programming problems considered during the course of optimization. We use
a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
7

Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
MWR are generated.
=0.1

=1

=10

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.266
0.0426

0.372
0.0532
0.777
0.0745

0.585
0.0745
0.904
0.0745

0.894
0.106
0.968
0.138

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.319
0.0532

0.394
0.0638
0.819
0.0745

0.606
0.0745
0.947
0.106

0.904
0.16
0.979
0.17

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.202
0.0532
0.447
0.0638

0.426
0.0957
0.936
0.128

0.628
0.128
0.979
0.181

0.915
0.223
0.989
0.287

We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
minimum vertex cover problem can be solved approximately or greedily.
In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
the exact value of τ does not effect the speed of optimization dramatically. We show performance
with and without relying on parallel processing. Our parallel processing times assume that we
have one CPU for each subproblem. For the problem instances in our application the number of
subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits
of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
improvement, since the master problem consumes a larger proportion of total CPU time.
In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
instances, the total CPU time required when using no MWR was prohibitively large, which is not the
case when MWR are employed. Thus most problem instances solved without MWR terminated early.
In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
the difference between the anytime solution (upper bound) and the lower bound on the objective. For
each such tolerance , we compute the percentage of instances, for which the duality gap is less than
, after various amounts of time. We observe that the performance of optimization without MWR,
but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
demonstrates that, across the dataset, MWR are of greater importance than parallelization.

7

</corps> 
<conclusion>Conclusions

We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
expect will find further use in the application of combinatorial optimization to problems in computer
vision.
8

The exploitation of results from the domain of operations research may lead to improved variants
of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
problem so as to enforce integrality on a subset of the variables of the master problem.

</conclusion>
<acknowledgement></acknowledgement> 
<references>References
[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
(ICCV-11), pages 2611–2618, 2011.
[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
International Conference on Computer Vision (ECCV-12), 2012.
[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting
planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
238–247, 2002.
[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
solver for multicut partitioning. In CVPR, 2014.
[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
10.1007/978-3-319-46475-6_44.
[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.
[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations
research, 33(5):989–1007, 1985.
[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft
routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
1776–1781, 1966.
[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
399–404, 1956.
[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
Management science, 20(5):822–844, 1974.
[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
Research (volume 9), 1961.
[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
Springer, 2016.
[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
2015.

9

[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
image and mesh graphs by lifted multicuts. In ICCV, 2015.
[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.
In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
Mathematical Programming Computation, 1(1):43–67, 2009.
[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
model selection criteria. Operations research, 29(3):464–484, 1981.
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 769–776. ACM, 2009.
[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2969239.2969249.
[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:
Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
june 2007.
[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
IEEE Transactions on, 39(5):694–697, May 1990.
[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
CVPR, 2017.
[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
CVPR, 2015.
[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 652–666, 2018.
[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
Systems, 2015.
[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
preprint arXiv:1805.04958, 2018.
[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.
In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
microscopy. In ISBI, 2014.

10

A

APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.
Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
vi vj )s∈S } is constructed, for which
Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of
xs .
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E +

M

x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
M

∀(vi , vj ) ∈ E +

M

∀(vi , vj ) ∈ Es− , s ∈ S.

xs∗
vi vj = 0
xs∗
vi vj = 1

(10)

The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the
optimizing solution for f in subproblem s, given x, x∗ respectively.
x∗vi vj = xvi vj + max fvsi vj
s∈S

x∗vi vj = xvi vj − fvsi vj

∀(vi , vj ) ∈ E +

∀(vi , vj ) ∈ Es− , s ∈ S

fvs∗
= 0 ∀(vi , vj ) ∈ E +
i vj

(11)

fvs∗
= 0 ∀(vi , vj ) ∈ Es−
i vj
These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that
since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),
which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total
P
decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the
former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand
the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields
in an increase of the objective of the master problem by −φvi vj (1 − xn
vi vj ), while the objective of subproblem
s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

B

Line by Line Description of BDCC

We provide the line by line description of Alg. 1.
• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
• Line 2: Indicate that we have not solved the LP relaxation yet.
• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
integral solution is produced.
1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
indicated by done_lp=True.
2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.
– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
set Ẑ.
– Line 11: Indicate that a Benders row was added this iteration.
4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
solving the master problem for the remainder of the algorithm.
• Line 18 Return solution x.

11

C

Generating Feasible Integer Solutions Prior to Convergence

Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so
that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and
the relaxation is small. In this section we consider the production of feasible integer solutions, given the current
solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this
procedure as rounding.
Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
using x∗ below.
κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
κvi vj =

φvi vj x∗vi vj

∀(vi , vj ) ∈ E

(12)

−

∗

Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let
xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
vi vj = 1 if exactly
one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where
s∗
x0s
as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
vi vj = 1Es− (vi , vj ), is achieved using x
s∗
The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
then the solution produced below has cost equal to that of x∗ .
M

xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
M

s∗
x+
vi vj = max xvi vj
s∈S

M

s∗
x+
vi vj = xvi vj

∀(vi , vj ) ∈ E +

(13)

∀(vi , vj ) ∈ Es− , s ∈ S

The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to
integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.
We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+
by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of
edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
x∗ )
1: x+
vi vj = 0 ∀(vi , vj ) ∈ E
2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
4: for s ∈ S do
5:
xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
+
s
6:
x+
vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
+
7:
κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
8: end for
9: Return x+
• Line 1: Initialize x+ as the zero vector.
• Line 2-3: Set κ according to Eq. (12)
• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
2. Line 6: Cut edges in x+ that are cut in xs .
3. Line 7: Set φvi vj to zero for cut edges in x+ .
• Line 9: Return the solution x+
When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],
though we do not exploit its capacity to tackle non-submodular problems.

12

</references>
<discussion></discussion>
<titre></titre> 
<auteur></auteur>
<abstract>Abstract
We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We
reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders
decomposition formulation has many subproblems, each associated with a node in
the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
enforces the cycle inequalities corresponding to edges with negative (repulsive)
weights attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
approaches, theoretically allows for massive parallelization.

</abstract>
<introduction>Introduction

Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
on a sparse graph with real valued edge weights, where nodes correspond to observations and
weighted edges describe the affinity between pairs of nodes.
For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .
The magnitude of the weight is a function of the confidence of the classifier.
The CC cost function sums up the weights of the edges separating connected components (referred
to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
naturally as a function of the edge weights, rather than requiring an additional search over some
model order parameter describing the number of clusters (entities) [37].
Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
programming with cutting planes. They do not scale easily to large CC problem instances and are not
Preprint. Under review.

easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
CC for domains, where massively parallel computation could be employed.
In this paper we apply the classic Benders decomposition from operations research [10] to CC for
computer vision. Benders decomposition is commonly applied in operations research to solve mixed
integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The
block structure requires that no row of the constraint matrix of the MILP contains variables from
more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
proceeds with the master problem solving optimization over its variables. The subsequent solution
of the subproblems can be done in parallel and provides primal/dual solutions over their variables
conditioned on the solution to the master problem. The dual solutions to the subproblems provide
constraints to the master problem. Optimization continues until no further constraints are added to
the master problem.
Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
solving the subproblems not only provides a solution for their variables, but also a lower bound in the
form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
solution to the master problem.
Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
regarding the original objective of the subproblem.
Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].

</introduction> 
<corps>2

Related Work

Correlation clustering has been successfully applied to multiple problems in computer vision including
image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
on random sampling and only provides optimality bounds.
Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
[39], Andres et al. [3].
Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
quality of the solution.
Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
correspond to detections of objects and edges are associated with probabilities of co-association.The
work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
multi-person pose estimation using CC augmented with node labeling.
Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
fractional variables (flows of commodities from facilities to customers in a network) associated with
2

constraints. Benders decomposition reformulates optimization so as to use only the integer variables
and converts the fractional variables into constraints. These constraints are referred to as Benders
rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
use of MWR [23], which are more binding than the standard Benders rows.
Benders decomposition has recently been introduced to computer vision (though not for CC), for the
purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
modeled so as to admit efficient optimization, using column generation and Benders decomposition
jointly. The application of Benders decomposition in our paper is distinct regarding the problem
domain, the underlying integer program and the structure of the Benders subproblems.

3

Standard Correlation Clustering Formulation

In this section, we review the standard optimization formulation for CC [1], which corresponds to a
graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
binary edge labeling problem.
Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
min

x∈{0,1}|E|

s.t.

X
(vi ,vj )∈E −

X

X

−φvi vj (1 − xvi vj ) +

φ vi vj x vi vj

(CC1 )

(vi ,vj )∈E +

xvi vj ≥ xvic vjc

∀c ∈ C,

(1)

(vi ,vj )∈Ec+

where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
(vi , vj ) with xvi vj = 1 as a cut edge.
The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
components. We refer to the constraints in Eq. (1) as cycle inequalities.
Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
ˆ The
corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
cycle inequalities exist, after which the ILP must be solved in each iteration.
We should note that earlier work in CC for computer vision did not require that cycle inequalities
contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
or its LP relaxation.
In this section, we reviewed the baseline approach for solving CC in the computer vision community.
In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
specific solver of Andres et al. [1].
3

4

Benders Decomposition for Correlation Clustering

In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
to denote the subset of E + adjacent to vs .
In this section, we assume that we are provided with S, which can be produced greedily or using an
LP/ILP solver.
Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
number of edges in the subproblem s.
X
X
X
(CC1 )
(CC2 ) :
min
−φvi vj (1 − xvi vj ) +
φ vi vj x vi vj +
Q(φ, s, x),
x∈{0,1}|E|

(vi ,vj )∈E −

(vi ,vj )∈E +

s∈S

(CC2 )
where Q(φ, s, x) is defined as follows.
Q(φ, s, x)

=

min
s

x ∈{0,1}

s.t.

X
|s|

−φvi vj (1 − xsvi vj ) +

(vi ,vj )∈Es−

X

X

φvi vj xsvi vj

(2)

(vi ,vj )∈E +

xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

(vi ,vj )∈Ec+

We now construct a solution x∗ = {x∗vi vj , (xs∗
vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
as follows.
M

x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E + .

(3)
(4)

The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
vi vj and
is defined as follows.

1, if (vi , vj ) ∈ Es−
xs∗
=
(5)
vi vj
0, otherwise.
In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
vi vj )s∈S } is no greater than that of
{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all
s ∈ S.
It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
all s ∈ S.
Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
connected components that are adjacent to one another, not including the root node vs . Note, that
merging any pair of such components, does not increase the cost, since those components are not
separated by negative weight edges in subproblem s and so the result is still a partition.
Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
labeling formulation of min-cut, with the notation below.
4

We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
(
1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
s
f vi vj =
(6)
1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all
(vi , vj ) ∈ Es− .
Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
X
X
Q(φ, s, x) = smin
φvi vj fvsi vj −
φvs v fvss v
(7)
fv v ≥0
i j
(vi ,vj )∈E +
mv ≥0

(vs ,v)∈Es−

λ−
vi vj

:

mvi − mvj ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

λ+
vi vj

:

mvj − mvi ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

ψv−

:

xvs v − fvss v ≤ mv

∀(vs , v) ∈ Es− ,

ψv+

:

mv ≤ xvs v + fvss v

∀(vs , v) ∈ Es+ ,

This yields to the corresponding dual subproblem.
X
+
max −
(λ−
vi vj + λvi vj )xvi vj +
λ≥0
ψ≥0

s.t.

ψv+i

X

ψv− xvs v −

(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

X

ψv+ xvs v

(8)

(vs ,v)∈Es+

1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
X

X

+
(λ−
vi vj − λvi vj ) +

vj
(vi ,vj )∈(E + \Es+ )

φ vi vj

−
(λ+
vj vi − λvj vi ) ≥ 0

∀vi ∈ V − vs

vj
(vj ,vi )∈(E + \Es+ )

−φvs v − ψv− ≥ 0
φvs v − ψv+ ≥ 0
+
− (λ−
vi v j + λ vi vj ) ≥ 0

∀(vs , v) ∈ Es−
∀(vs , v) ∈ Es+
∀(vi , vj ) ∈ (E + \ Es+ ).

In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
xvi vj term.

+
−(λ−
if (vi , vj ) ∈ (E + \ Es+ )

vi vj + λvi vj ),




−ψv+j ,
if (vi , vj ) ∈ Es+
ωvzi vj =

ψv−j ,
if (vi , vj ) ∈ Es−




0,
if (vi , vj ) ∈ (E − \ Es− ).
We denote the set of all dual feasible solutions across s P
∈ S as Z, with z ∈ Z. Observe, that to
enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
We formulate CC as optimization using Z below.
X
X
(CC2 )
(CC3 ) = min
φ vi vj x vi vj −
(1 − xvi vj )φvi vj
(CC3 )
x∈{0,1}|E|

s.t.

X

(vi ,vj )∈E −

(vi ,vj )∈E +

xvi vj ωvzi vj ≤ 0

∀z ∈ Z

(vi ,vj )∈E

5

Algorithm 1 Benders Decomposition for CC (BDCC)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

4.1

Ẑ = {}
done_LP = False
repeat
x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
did_add = False
for s ∈ S do
if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then
z1 = Get Benders row via Eq (8).
z2 = Get MWR via Sec. 5.
Ẑ = Ẑ ∪ z1 ∪ z2
did_add = True
end if
end for
if did_add=False then
done_LP = True
end if
until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
Return x

Cutting Plane Optimization

Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
master problem) and generating new Benders rows until no violated constraints exist.
This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then
we have identified a violated cycle inequality associated with s.
We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
rows, which we describe in the following Sec. 5.
Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
1
provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
vi vj = 1, if xvi vj > 2
∗
and otherwise set x∗∗
vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
∗∗
connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
(supplementary material), we provide a more involved approach to produce feasible integer solutions.
In this section, we characterized CC using Benders decomposition and provided a cutting plane
algorithm to solve the corresponding optimization.

5

Magnanti-Wong Benders Rows

We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
6

Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
and black for not using Magnanti-Wong rows. We show both the computation time with and without
exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
total running time versus the total running time when solving each subproblem is done on its own
CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
where we replace the objective and add one additional constraint.
We follow the tradition of the operations research literature and use a random negative valued vector
(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
−1
subproblem is solved. We experimented with using as an objective .0001+|φ
, which encourages
vi vj |
the cutting of edges with large positive weight, but it works as well as the random negative objective.
Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
X
X
X
+
τ Q(φ, s, x) ≤ −
(λ−
ψv− xvs v −
ψv+ xvs v
vi vj + λvi vj )xvi vj +
(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

(vs ,v)∈Es+

(9)
Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
experiments, we found that τ = 12 provides strong performance.

6

Experiments: Image Segmentation

In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates
optimization.
To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
linear and integer linear programming problems considered during the course of optimization. We use
a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
7

Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
MWR are generated.
=0.1

=1

=10

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.266
0.0426

0.372
0.0532
0.777
0.0745

0.585
0.0745
0.904
0.0745

0.894
0.106
0.968
0.138

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.319
0.0532

0.394
0.0638
0.819
0.0745

0.606
0.0745
0.947
0.106

0.904
0.16
0.979
0.17

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.202
0.0532
0.447
0.0638

0.426
0.0957
0.936
0.128

0.628
0.128
0.979
0.181

0.915
0.223
0.989
0.287

We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
minimum vertex cover problem can be solved approximately or greedily.
In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
the exact value of τ does not effect the speed of optimization dramatically. We show performance
with and without relying on parallel processing. Our parallel processing times assume that we
have one CPU for each subproblem. For the problem instances in our application the number of
subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits
of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
improvement, since the master problem consumes a larger proportion of total CPU time.
In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
instances, the total CPU time required when using no MWR was prohibitively large, which is not the
case when MWR are employed. Thus most problem instances solved without MWR terminated early.
In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
the difference between the anytime solution (upper bound) and the lower bound on the objective. For
each such tolerance , we compute the percentage of instances, for which the duality gap is less than
, after various amounts of time. We observe that the performance of optimization without MWR,
but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
demonstrates that, across the dataset, MWR are of greater importance than parallelization.

7

</corps> 
<conclusion>Conclusions

We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
expect will find further use in the application of combinatorial optimization to problems in computer
vision.
8

The exploitation of results from the domain of operations research may lead to improved variants
of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
problem so as to enforce integrality on a subset of the variables of the master problem.

</conclusion>
<acknowledgement></acknowledgement> 
<references>References
[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
(ICCV-11), pages 2611–2618, 2011.
[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
International Conference on Computer Vision (ECCV-12), 2012.
[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting
planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
238–247, 2002.
[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
solver for multicut partitioning. In CVPR, 2014.
[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
10.1007/978-3-319-46475-6_44.
[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.
[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations
research, 33(5):989–1007, 1985.
[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft
routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
1776–1781, 1966.
[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
399–404, 1956.
[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
Management science, 20(5):822–844, 1974.
[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
Research (volume 9), 1961.
[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
Springer, 2016.
[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
2015.

9

[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
image and mesh graphs by lifted multicuts. In ICCV, 2015.
[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.
In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
Mathematical Programming Computation, 1(1):43–67, 2009.
[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
model selection criteria. Operations research, 29(3):464–484, 1981.
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 769–776. ACM, 2009.
[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2969239.2969249.
[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:
Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
june 2007.
[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
IEEE Transactions on, 39(5):694–697, May 1990.
[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
CVPR, 2017.
[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
CVPR, 2015.
[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 652–666, 2018.
[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
Systems, 2015.
[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
preprint arXiv:1805.04958, 2018.
[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.
In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
microscopy. In ISBI, 2014.

10

A

APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.
Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
vi vj )s∈S } is constructed, for which
Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of
xs .
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E +

M

x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
M

∀(vi , vj ) ∈ E +

M

∀(vi , vj ) ∈ Es− , s ∈ S.

xs∗
vi vj = 0
xs∗
vi vj = 1

(10)

The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the
optimizing solution for f in subproblem s, given x, x∗ respectively.
x∗vi vj = xvi vj + max fvsi vj
s∈S

x∗vi vj = xvi vj − fvsi vj

∀(vi , vj ) ∈ E +

∀(vi , vj ) ∈ Es− , s ∈ S

fvs∗
= 0 ∀(vi , vj ) ∈ E +
i vj

(11)

fvs∗
= 0 ∀(vi , vj ) ∈ Es−
i vj
These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that
since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),
which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total
P
decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the
former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand
the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields
in an increase of the objective of the master problem by −φvi vj (1 − xn
vi vj ), while the objective of subproblem
s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

B

Line by Line Description of BDCC

We provide the line by line description of Alg. 1.
• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
• Line 2: Indicate that we have not solved the LP relaxation yet.
• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
integral solution is produced.
1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
indicated by done_lp=True.
2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.
– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
set Ẑ.
– Line 11: Indicate that a Benders row was added this iteration.
4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
solving the master problem for the remainder of the algorithm.
• Line 18 Return solution x.

11

C

Generating Feasible Integer Solutions Prior to Convergence

Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so
that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and
the relaxation is small. In this section we consider the production of feasible integer solutions, given the current
solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this
procedure as rounding.
Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
using x∗ below.
κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
κvi vj =

φvi vj x∗vi vj

∀(vi , vj ) ∈ E

(12)

−

∗

Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let
xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
vi vj = 1 if exactly
one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where
s∗
x0s
as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
vi vj = 1Es− (vi , vj ), is achieved using x
s∗
The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
then the solution produced below has cost equal to that of x∗ .
M

xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
M

s∗
x+
vi vj = max xvi vj
s∈S

M

s∗
x+
vi vj = xvi vj

∀(vi , vj ) ∈ E +

(13)

∀(vi , vj ) ∈ Es− , s ∈ S

The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to
integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.
We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+
by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of
edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
x∗ )
1: x+
vi vj = 0 ∀(vi , vj ) ∈ E
2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
4: for s ∈ S do
5:
xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
+
s
6:
x+
vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
+
7:
κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
8: end for
9: Return x+
• Line 1: Initialize x+ as the zero vector.
• Line 2-3: Set κ according to Eq. (12)
• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
2. Line 6: Cut edges in x+ that are cut in xs .
3. Line 7: Set φvi vj to zero for cut edges in x+ .
• Line 9: Return the solution x+
When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],
though we do not exploit its capacity to tackle non-submodular problems.

12

</references>
<discussion></discussion>
<titre></titre> 
<auteur></auteur>
<abstract>Abstract
We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We
reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders
decomposition formulation has many subproblems, each associated with a node in
the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
enforces the cycle inequalities corresponding to edges with negative (repulsive)
weights attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
approaches, theoretically allows for massive parallelization.

</abstract>
<introduction>Introduction

Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
on a sparse graph with real valued edge weights, where nodes correspond to observations and
weighted edges describe the affinity between pairs of nodes.
For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .
The magnitude of the weight is a function of the confidence of the classifier.
The CC cost function sums up the weights of the edges separating connected components (referred
to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
naturally as a function of the edge weights, rather than requiring an additional search over some
model order parameter describing the number of clusters (entities) [37].
Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
programming with cutting planes. They do not scale easily to large CC problem instances and are not
Preprint. Under review.

easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
CC for domains, where massively parallel computation could be employed.
In this paper we apply the classic Benders decomposition from operations research [10] to CC for
computer vision. Benders decomposition is commonly applied in operations research to solve mixed
integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The
block structure requires that no row of the constraint matrix of the MILP contains variables from
more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
proceeds with the master problem solving optimization over its variables. The subsequent solution
of the subproblems can be done in parallel and provides primal/dual solutions over their variables
conditioned on the solution to the master problem. The dual solutions to the subproblems provide
constraints to the master problem. Optimization continues until no further constraints are added to
the master problem.
Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
solving the subproblems not only provides a solution for their variables, but also a lower bound in the
form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
solution to the master problem.
Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
regarding the original objective of the subproblem.
Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].

</introduction> 
<corps>2

Related Work

Correlation clustering has been successfully applied to multiple problems in computer vision including
image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
on random sampling and only provides optimality bounds.
Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
[39], Andres et al. [3].
Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
quality of the solution.
Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
correspond to detections of objects and edges are associated with probabilities of co-association.The
work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
multi-person pose estimation using CC augmented with node labeling.
Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
fractional variables (flows of commodities from facilities to customers in a network) associated with
2

constraints. Benders decomposition reformulates optimization so as to use only the integer variables
and converts the fractional variables into constraints. These constraints are referred to as Benders
rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
use of MWR [23], which are more binding than the standard Benders rows.
Benders decomposition has recently been introduced to computer vision (though not for CC), for the
purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
modeled so as to admit efficient optimization, using column generation and Benders decomposition
jointly. The application of Benders decomposition in our paper is distinct regarding the problem
domain, the underlying integer program and the structure of the Benders subproblems.

3

Standard Correlation Clustering Formulation

In this section, we review the standard optimization formulation for CC [1], which corresponds to a
graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
binary edge labeling problem.
Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
min

x∈{0,1}|E|

s.t.

X
(vi ,vj )∈E −

X

X

−φvi vj (1 − xvi vj ) +

φ vi vj x vi vj

(CC1 )

(vi ,vj )∈E +

xvi vj ≥ xvic vjc

∀c ∈ C,

(1)

(vi ,vj )∈Ec+

where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
(vi , vj ) with xvi vj = 1 as a cut edge.
The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
components. We refer to the constraints in Eq. (1) as cycle inequalities.
Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
ˆ The
corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
cycle inequalities exist, after which the ILP must be solved in each iteration.
We should note that earlier work in CC for computer vision did not require that cycle inequalities
contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
or its LP relaxation.
In this section, we reviewed the baseline approach for solving CC in the computer vision community.
In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
specific solver of Andres et al. [1].
3

4

Benders Decomposition for Correlation Clustering

In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
to denote the subset of E + adjacent to vs .
In this section, we assume that we are provided with S, which can be produced greedily or using an
LP/ILP solver.
Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
number of edges in the subproblem s.
X
X
X
(CC1 )
(CC2 ) :
min
−φvi vj (1 − xvi vj ) +
φ vi vj x vi vj +
Q(φ, s, x),
x∈{0,1}|E|

(vi ,vj )∈E −

(vi ,vj )∈E +

s∈S

(CC2 )
where Q(φ, s, x) is defined as follows.
Q(φ, s, x)

=

min
s

x ∈{0,1}

s.t.

X
|s|

−φvi vj (1 − xsvi vj ) +

(vi ,vj )∈Es−

X

X

φvi vj xsvi vj

(2)

(vi ,vj )∈E +

xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

(vi ,vj )∈Ec+

We now construct a solution x∗ = {x∗vi vj , (xs∗
vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
as follows.
M

x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E + .

(3)
(4)

The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
vi vj and
is defined as follows.

1, if (vi , vj ) ∈ Es−
xs∗
=
(5)
vi vj
0, otherwise.
In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
vi vj )s∈S } is no greater than that of
{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all
s ∈ S.
It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
all s ∈ S.
Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
connected components that are adjacent to one another, not including the root node vs . Note, that
merging any pair of such components, does not increase the cost, since those components are not
separated by negative weight edges in subproblem s and so the result is still a partition.
Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
labeling formulation of min-cut, with the notation below.
4

We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
(
1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
s
f vi vj =
(6)
1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all
(vi , vj ) ∈ Es− .
Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
X
X
Q(φ, s, x) = smin
φvi vj fvsi vj −
φvs v fvss v
(7)
fv v ≥0
i j
(vi ,vj )∈E +
mv ≥0

(vs ,v)∈Es−

λ−
vi vj

:

mvi − mvj ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

λ+
vi vj

:

mvj − mvi ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

ψv−

:

xvs v − fvss v ≤ mv

∀(vs , v) ∈ Es− ,

ψv+

:

mv ≤ xvs v + fvss v

∀(vs , v) ∈ Es+ ,

This yields to the corresponding dual subproblem.
X
+
max −
(λ−
vi vj + λvi vj )xvi vj +
λ≥0
ψ≥0

s.t.

ψv+i

X

ψv− xvs v −

(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

X

ψv+ xvs v

(8)

(vs ,v)∈Es+

1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
X

X

+
(λ−
vi vj − λvi vj ) +

vj
(vi ,vj )∈(E + \Es+ )

φ vi vj

−
(λ+
vj vi − λvj vi ) ≥ 0

∀vi ∈ V − vs

vj
(vj ,vi )∈(E + \Es+ )

−φvs v − ψv− ≥ 0
φvs v − ψv+ ≥ 0
+
− (λ−
vi v j + λ vi vj ) ≥ 0

∀(vs , v) ∈ Es−
∀(vs , v) ∈ Es+
∀(vi , vj ) ∈ (E + \ Es+ ).

In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
xvi vj term.

+
−(λ−
if (vi , vj ) ∈ (E + \ Es+ )

vi vj + λvi vj ),




−ψv+j ,
if (vi , vj ) ∈ Es+
ωvzi vj =

ψv−j ,
if (vi , vj ) ∈ Es−




0,
if (vi , vj ) ∈ (E − \ Es− ).
We denote the set of all dual feasible solutions across s P
∈ S as Z, with z ∈ Z. Observe, that to
enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
We formulate CC as optimization using Z below.
X
X
(CC2 )
(CC3 ) = min
φ vi vj x vi vj −
(1 − xvi vj )φvi vj
(CC3 )
x∈{0,1}|E|

s.t.

X

(vi ,vj )∈E −

(vi ,vj )∈E +

xvi vj ωvzi vj ≤ 0

∀z ∈ Z

(vi ,vj )∈E

5

Algorithm 1 Benders Decomposition for CC (BDCC)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

4.1

Ẑ = {}
done_LP = False
repeat
x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
did_add = False
for s ∈ S do
if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then
z1 = Get Benders row via Eq (8).
z2 = Get MWR via Sec. 5.
Ẑ = Ẑ ∪ z1 ∪ z2
did_add = True
end if
end for
if did_add=False then
done_LP = True
end if
until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
Return x

Cutting Plane Optimization

Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
master problem) and generating new Benders rows until no violated constraints exist.
This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then
we have identified a violated cycle inequality associated with s.
We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
rows, which we describe in the following Sec. 5.
Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
1
provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
vi vj = 1, if xvi vj > 2
∗
and otherwise set x∗∗
vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
∗∗
connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
(supplementary material), we provide a more involved approach to produce feasible integer solutions.
In this section, we characterized CC using Benders decomposition and provided a cutting plane
algorithm to solve the corresponding optimization.

5

Magnanti-Wong Benders Rows

We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
6

Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
and black for not using Magnanti-Wong rows. We show both the computation time with and without
exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
total running time versus the total running time when solving each subproblem is done on its own
CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
where we replace the objective and add one additional constraint.
We follow the tradition of the operations research literature and use a random negative valued vector
(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
−1
subproblem is solved. We experimented with using as an objective .0001+|φ
, which encourages
vi vj |
the cutting of edges with large positive weight, but it works as well as the random negative objective.
Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
X
X
X
+
τ Q(φ, s, x) ≤ −
(λ−
ψv− xvs v −
ψv+ xvs v
vi vj + λvi vj )xvi vj +
(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

(vs ,v)∈Es+

(9)
Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
experiments, we found that τ = 12 provides strong performance.

6

Experiments: Image Segmentation

In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates
optimization.
To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
linear and integer linear programming problems considered during the course of optimization. We use
a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
7

Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
MWR are generated.
=0.1

=1

=10

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.266
0.0426

0.372
0.0532
0.777
0.0745

0.585
0.0745
0.904
0.0745

0.894
0.106
0.968
0.138

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.319
0.0532

0.394
0.0638
0.819
0.0745

0.606
0.0745
0.947
0.106

0.904
0.16
0.979
0.17

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.202
0.0532
0.447
0.0638

0.426
0.0957
0.936
0.128

0.628
0.128
0.979
0.181

0.915
0.223
0.989
0.287

We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
minimum vertex cover problem can be solved approximately or greedily.
In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
the exact value of τ does not effect the speed of optimization dramatically. We show performance
with and without relying on parallel processing. Our parallel processing times assume that we
have one CPU for each subproblem. For the problem instances in our application the number of
subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits
of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
improvement, since the master problem consumes a larger proportion of total CPU time.
In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
instances, the total CPU time required when using no MWR was prohibitively large, which is not the
case when MWR are employed. Thus most problem instances solved without MWR terminated early.
In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
the difference between the anytime solution (upper bound) and the lower bound on the objective. For
each such tolerance , we compute the percentage of instances, for which the duality gap is less than
, after various amounts of time. We observe that the performance of optimization without MWR,
but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
demonstrates that, across the dataset, MWR are of greater importance than parallelization.

7

</corps> 
<conclusion>Conclusions

We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
expect will find further use in the application of combinatorial optimization to problems in computer
vision.
8

The exploitation of results from the domain of operations research may lead to improved variants
of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
problem so as to enforce integrality on a subset of the variables of the master problem.

</conclusion>
<acknowledgement></acknowledgement> 
<references>References
[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
(ICCV-11), pages 2611–2618, 2011.
[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
International Conference on Computer Vision (ECCV-12), 2012.
[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting
planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
238–247, 2002.
[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
solver for multicut partitioning. In CVPR, 2014.
[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
10.1007/978-3-319-46475-6_44.
[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.
[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations
research, 33(5):989–1007, 1985.
[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft
routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
1776–1781, 1966.
[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
399–404, 1956.
[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
Management science, 20(5):822–844, 1974.
[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
Research (volume 9), 1961.
[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
Springer, 2016.
[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
2015.

9

[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
image and mesh graphs by lifted multicuts. In ICCV, 2015.
[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.
In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
Mathematical Programming Computation, 1(1):43–67, 2009.
[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
model selection criteria. Operations research, 29(3):464–484, 1981.
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 769–776. ACM, 2009.
[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2969239.2969249.
[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:
Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
june 2007.
[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
IEEE Transactions on, 39(5):694–697, May 1990.
[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
CVPR, 2017.
[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
CVPR, 2015.
[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 652–666, 2018.
[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
Systems, 2015.
[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
preprint arXiv:1805.04958, 2018.
[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.
In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
microscopy. In ISBI, 2014.

10

A

APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.
Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
vi vj )s∈S } is constructed, for which
Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of
xs .
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E +

M

x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
M

∀(vi , vj ) ∈ E +

M

∀(vi , vj ) ∈ Es− , s ∈ S.

xs∗
vi vj = 0
xs∗
vi vj = 1

(10)

The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the
optimizing solution for f in subproblem s, given x, x∗ respectively.
x∗vi vj = xvi vj + max fvsi vj
s∈S

x∗vi vj = xvi vj − fvsi vj

∀(vi , vj ) ∈ E +

∀(vi , vj ) ∈ Es− , s ∈ S

fvs∗
= 0 ∀(vi , vj ) ∈ E +
i vj

(11)

fvs∗
= 0 ∀(vi , vj ) ∈ Es−
i vj
These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that
since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),
which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total
P
decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the
former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand
the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields
in an increase of the objective of the master problem by −φvi vj (1 − xn
vi vj ), while the objective of subproblem
s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

B

Line by Line Description of BDCC

We provide the line by line description of Alg. 1.
• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
• Line 2: Indicate that we have not solved the LP relaxation yet.
• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
integral solution is produced.
1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
indicated by done_lp=True.
2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.
– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
set Ẑ.
– Line 11: Indicate that a Benders row was added this iteration.
4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
solving the master problem for the remainder of the algorithm.
• Line 18 Return solution x.

11

C

Generating Feasible Integer Solutions Prior to Convergence

Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so
that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and
the relaxation is small. In this section we consider the production of feasible integer solutions, given the current
solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this
procedure as rounding.
Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
using x∗ below.
κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
κvi vj =

φvi vj x∗vi vj

∀(vi , vj ) ∈ E

(12)

−

∗

Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let
xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
vi vj = 1 if exactly
one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where
s∗
x0s
as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
vi vj = 1Es− (vi , vj ), is achieved using x
s∗
The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
then the solution produced below has cost equal to that of x∗ .
M

xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
M

s∗
x+
vi vj = max xvi vj
s∈S

M

s∗
x+
vi vj = xvi vj

∀(vi , vj ) ∈ E +

(13)

∀(vi , vj ) ∈ Es− , s ∈ S

The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to
integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.
We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+
by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of
edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
x∗ )
1: x+
vi vj = 0 ∀(vi , vj ) ∈ E
2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
4: for s ∈ S do
5:
xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
+
s
6:
x+
vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
+
7:
κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
8: end for
9: Return x+
• Line 1: Initialize x+ as the zero vector.
• Line 2-3: Set κ according to Eq. (12)
• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
2. Line 6: Cut edges in x+ that are cut in xs .
3. Line 7: Set φvi vj to zero for cut edges in x+ .
• Line 9: Return the solution x+
When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],
though we do not exploit its capacity to tackle non-submodular problems.

12

</references>
<discussion></discussion>
<titre></titre> 
<auteur></auteur>
<abstract>Abstract
We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We
reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders
decomposition formulation has many subproblems, each associated with a node in
the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
enforces the cycle inequalities corresponding to edges with negative (repulsive)
weights attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
approaches, theoretically allows for massive parallelization.

</abstract>
<introduction>Introduction

Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
on a sparse graph with real valued edge weights, where nodes correspond to observations and
weighted edges describe the affinity between pairs of nodes.
For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .
The magnitude of the weight is a function of the confidence of the classifier.
The CC cost function sums up the weights of the edges separating connected components (referred
to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
naturally as a function of the edge weights, rather than requiring an additional search over some
model order parameter describing the number of clusters (entities) [37].
Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
programming with cutting planes. They do not scale easily to large CC problem instances and are not
Preprint. Under review.

easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
CC for domains, where massively parallel computation could be employed.
In this paper we apply the classic Benders decomposition from operations research [10] to CC for
computer vision. Benders decomposition is commonly applied in operations research to solve mixed
integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The
block structure requires that no row of the constraint matrix of the MILP contains variables from
more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
proceeds with the master problem solving optimization over its variables. The subsequent solution
of the subproblems can be done in parallel and provides primal/dual solutions over their variables
conditioned on the solution to the master problem. The dual solutions to the subproblems provide
constraints to the master problem. Optimization continues until no further constraints are added to
the master problem.
Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
solving the subproblems not only provides a solution for their variables, but also a lower bound in the
form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
solution to the master problem.
Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
regarding the original objective of the subproblem.
Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].

</introduction> 
<corps>2

Related Work

Correlation clustering has been successfully applied to multiple problems in computer vision including
image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
on random sampling and only provides optimality bounds.
Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
[39], Andres et al. [3].
Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
quality of the solution.
Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
correspond to detections of objects and edges are associated with probabilities of co-association.The
work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
multi-person pose estimation using CC augmented with node labeling.
Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
fractional variables (flows of commodities from facilities to customers in a network) associated with
2

constraints. Benders decomposition reformulates optimization so as to use only the integer variables
and converts the fractional variables into constraints. These constraints are referred to as Benders
rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
use of MWR [23], which are more binding than the standard Benders rows.
Benders decomposition has recently been introduced to computer vision (though not for CC), for the
purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
modeled so as to admit efficient optimization, using column generation and Benders decomposition
jointly. The application of Benders decomposition in our paper is distinct regarding the problem
domain, the underlying integer program and the structure of the Benders subproblems.

3

Standard Correlation Clustering Formulation

In this section, we review the standard optimization formulation for CC [1], which corresponds to a
graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
binary edge labeling problem.
Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
min

x∈{0,1}|E|

s.t.

X
(vi ,vj )∈E −

X

X

−φvi vj (1 − xvi vj ) +

φ vi vj x vi vj

(CC1 )

(vi ,vj )∈E +

xvi vj ≥ xvic vjc

∀c ∈ C,

(1)

(vi ,vj )∈Ec+

where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
(vi , vj ) with xvi vj = 1 as a cut edge.
The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
components. We refer to the constraints in Eq. (1) as cycle inequalities.
Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
ˆ The
corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
cycle inequalities exist, after which the ILP must be solved in each iteration.
We should note that earlier work in CC for computer vision did not require that cycle inequalities
contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
or its LP relaxation.
In this section, we reviewed the baseline approach for solving CC in the computer vision community.
In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
specific solver of Andres et al. [1].
3

4

Benders Decomposition for Correlation Clustering

In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
to denote the subset of E + adjacent to vs .
In this section, we assume that we are provided with S, which can be produced greedily or using an
LP/ILP solver.
Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
number of edges in the subproblem s.
X
X
X
(CC1 )
(CC2 ) :
min
−φvi vj (1 − xvi vj ) +
φ vi vj x vi vj +
Q(φ, s, x),
x∈{0,1}|E|

(vi ,vj )∈E −

(vi ,vj )∈E +

s∈S

(CC2 )
where Q(φ, s, x) is defined as follows.
Q(φ, s, x)

=

min
s

x ∈{0,1}

s.t.

X
|s|

−φvi vj (1 − xsvi vj ) +

(vi ,vj )∈Es−

X

X

φvi vj xsvi vj

(2)

(vi ,vj )∈E +

xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

(vi ,vj )∈Ec+

We now construct a solution x∗ = {x∗vi vj , (xs∗
vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
as follows.
M

x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E + .

(3)
(4)

The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
vi vj and
is defined as follows.

1, if (vi , vj ) ∈ Es−
xs∗
=
(5)
vi vj
0, otherwise.
In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
vi vj )s∈S } is no greater than that of
{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all
s ∈ S.
It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
all s ∈ S.
Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
connected components that are adjacent to one another, not including the root node vs . Note, that
merging any pair of such components, does not increase the cost, since those components are not
separated by negative weight edges in subproblem s and so the result is still a partition.
Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
labeling formulation of min-cut, with the notation below.
4

We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
(
1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
s
f vi vj =
(6)
1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all
(vi , vj ) ∈ Es− .
Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
X
X
Q(φ, s, x) = smin
φvi vj fvsi vj −
φvs v fvss v
(7)
fv v ≥0
i j
(vi ,vj )∈E +
mv ≥0

(vs ,v)∈Es−

λ−
vi vj

:

mvi − mvj ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

λ+
vi vj

:

mvj − mvi ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

ψv−

:

xvs v − fvss v ≤ mv

∀(vs , v) ∈ Es− ,

ψv+

:

mv ≤ xvs v + fvss v

∀(vs , v) ∈ Es+ ,

This yields to the corresponding dual subproblem.
X
+
max −
(λ−
vi vj + λvi vj )xvi vj +
λ≥0
ψ≥0

s.t.

ψv+i

X

ψv− xvs v −

(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

X

ψv+ xvs v

(8)

(vs ,v)∈Es+

1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
X

X

+
(λ−
vi vj − λvi vj ) +

vj
(vi ,vj )∈(E + \Es+ )

φ vi vj

−
(λ+
vj vi − λvj vi ) ≥ 0

∀vi ∈ V − vs

vj
(vj ,vi )∈(E + \Es+ )

−φvs v − ψv− ≥ 0
φvs v − ψv+ ≥ 0
+
− (λ−
vi v j + λ vi vj ) ≥ 0

∀(vs , v) ∈ Es−
∀(vs , v) ∈ Es+
∀(vi , vj ) ∈ (E + \ Es+ ).

In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
xvi vj term.

+
−(λ−
if (vi , vj ) ∈ (E + \ Es+ )

vi vj + λvi vj ),




−ψv+j ,
if (vi , vj ) ∈ Es+
ωvzi vj =

ψv−j ,
if (vi , vj ) ∈ Es−




0,
if (vi , vj ) ∈ (E − \ Es− ).
We denote the set of all dual feasible solutions across s P
∈ S as Z, with z ∈ Z. Observe, that to
enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
We formulate CC as optimization using Z below.
X
X
(CC2 )
(CC3 ) = min
φ vi vj x vi vj −
(1 − xvi vj )φvi vj
(CC3 )
x∈{0,1}|E|

s.t.

X

(vi ,vj )∈E −

(vi ,vj )∈E +

xvi vj ωvzi vj ≤ 0

∀z ∈ Z

(vi ,vj )∈E

5

Algorithm 1 Benders Decomposition for CC (BDCC)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

4.1

Ẑ = {}
done_LP = False
repeat
x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
did_add = False
for s ∈ S do
if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then
z1 = Get Benders row via Eq (8).
z2 = Get MWR via Sec. 5.
Ẑ = Ẑ ∪ z1 ∪ z2
did_add = True
end if
end for
if did_add=False then
done_LP = True
end if
until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
Return x

Cutting Plane Optimization

Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
master problem) and generating new Benders rows until no violated constraints exist.
This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then
we have identified a violated cycle inequality associated with s.
We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
rows, which we describe in the following Sec. 5.
Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
1
provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
vi vj = 1, if xvi vj > 2
∗
and otherwise set x∗∗
vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
∗∗
connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
(supplementary material), we provide a more involved approach to produce feasible integer solutions.
In this section, we characterized CC using Benders decomposition and provided a cutting plane
algorithm to solve the corresponding optimization.

5

Magnanti-Wong Benders Rows

We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
6

Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
and black for not using Magnanti-Wong rows. We show both the computation time with and without
exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
total running time versus the total running time when solving each subproblem is done on its own
CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
where we replace the objective and add one additional constraint.
We follow the tradition of the operations research literature and use a random negative valued vector
(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
−1
subproblem is solved. We experimented with using as an objective .0001+|φ
, which encourages
vi vj |
the cutting of edges with large positive weight, but it works as well as the random negative objective.
Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
X
X
X
+
τ Q(φ, s, x) ≤ −
(λ−
ψv− xvs v −
ψv+ xvs v
vi vj + λvi vj )xvi vj +
(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

(vs ,v)∈Es+

(9)
Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
experiments, we found that τ = 12 provides strong performance.

6

Experiments: Image Segmentation

In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates
optimization.
To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
linear and integer linear programming problems considered during the course of optimization. We use
a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
7

Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
MWR are generated.
=0.1

=1

=10

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.266
0.0426

0.372
0.0532
0.777
0.0745

0.585
0.0745
0.904
0.0745

0.894
0.106
0.968
0.138

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.319
0.0532

0.394
0.0638
0.819
0.0745

0.606
0.0745
0.947
0.106

0.904
0.16
0.979
0.17

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.202
0.0532
0.447
0.0638

0.426
0.0957
0.936
0.128

0.628
0.128
0.979
0.181

0.915
0.223
0.989
0.287

We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
minimum vertex cover problem can be solved approximately or greedily.
In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
the exact value of τ does not effect the speed of optimization dramatically. We show performance
with and without relying on parallel processing. Our parallel processing times assume that we
have one CPU for each subproblem. For the problem instances in our application the number of
subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits
of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
improvement, since the master problem consumes a larger proportion of total CPU time.
In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
instances, the total CPU time required when using no MWR was prohibitively large, which is not the
case when MWR are employed. Thus most problem instances solved without MWR terminated early.
In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
the difference between the anytime solution (upper bound) and the lower bound on the objective. For
each such tolerance , we compute the percentage of instances, for which the duality gap is less than
, after various amounts of time. We observe that the performance of optimization without MWR,
but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
demonstrates that, across the dataset, MWR are of greater importance than parallelization.

7

</corps> 
<conclusion>Conclusions

We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
expect will find further use in the application of combinatorial optimization to problems in computer
vision.
8

The exploitation of results from the domain of operations research may lead to improved variants
of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
problem so as to enforce integrality on a subset of the variables of the master problem.

</conclusion>
<acknowledgement></acknowledgement> 
<references>References
[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
(ICCV-11), pages 2611–2618, 2011.
[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
International Conference on Computer Vision (ECCV-12), 2012.
[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting
planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
238–247, 2002.
[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
solver for multicut partitioning. In CVPR, 2014.
[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
10.1007/978-3-319-46475-6_44.
[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.
[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations
research, 33(5):989–1007, 1985.
[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft
routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
1776–1781, 1966.
[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
399–404, 1956.
[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
Management science, 20(5):822–844, 1974.
[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
Research (volume 9), 1961.
[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
Springer, 2016.
[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
2015.

9

[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
image and mesh graphs by lifted multicuts. In ICCV, 2015.
[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.
In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
Mathematical Programming Computation, 1(1):43–67, 2009.
[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
model selection criteria. Operations research, 29(3):464–484, 1981.
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 769–776. ACM, 2009.
[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2969239.2969249.
[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:
Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
june 2007.
[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
IEEE Transactions on, 39(5):694–697, May 1990.
[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
CVPR, 2017.
[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
CVPR, 2015.
[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 652–666, 2018.
[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
Systems, 2015.
[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
preprint arXiv:1805.04958, 2018.
[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.
In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
microscopy. In ISBI, 2014.

10

A

APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.
Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
vi vj )s∈S } is constructed, for which
Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of
xs .
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E +

M

x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
M

∀(vi , vj ) ∈ E +

M

∀(vi , vj ) ∈ Es− , s ∈ S.

xs∗
vi vj = 0
xs∗
vi vj = 1

(10)

The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the
optimizing solution for f in subproblem s, given x, x∗ respectively.
x∗vi vj = xvi vj + max fvsi vj
s∈S

x∗vi vj = xvi vj − fvsi vj

∀(vi , vj ) ∈ E +

∀(vi , vj ) ∈ Es− , s ∈ S

fvs∗
= 0 ∀(vi , vj ) ∈ E +
i vj

(11)

fvs∗
= 0 ∀(vi , vj ) ∈ Es−
i vj
These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that
since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),
which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total
P
decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the
former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand
the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields
in an increase of the objective of the master problem by −φvi vj (1 − xn
vi vj ), while the objective of subproblem
s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

B

Line by Line Description of BDCC

We provide the line by line description of Alg. 1.
• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
• Line 2: Indicate that we have not solved the LP relaxation yet.
• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
integral solution is produced.
1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
indicated by done_lp=True.
2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.
– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
set Ẑ.
– Line 11: Indicate that a Benders row was added this iteration.
4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
solving the master problem for the remainder of the algorithm.
• Line 18 Return solution x.

11

C

Generating Feasible Integer Solutions Prior to Convergence

Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so
that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and
the relaxation is small. In this section we consider the production of feasible integer solutions, given the current
solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this
procedure as rounding.
Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
using x∗ below.
κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
κvi vj =

φvi vj x∗vi vj

∀(vi , vj ) ∈ E

(12)

−

∗

Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let
xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
vi vj = 1 if exactly
one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where
s∗
x0s
as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
vi vj = 1Es− (vi , vj ), is achieved using x
s∗
The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
then the solution produced below has cost equal to that of x∗ .
M

xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
M

s∗
x+
vi vj = max xvi vj
s∈S

M

s∗
x+
vi vj = xvi vj

∀(vi , vj ) ∈ E +

(13)

∀(vi , vj ) ∈ Es− , s ∈ S

The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to
integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.
We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+
by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of
edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
x∗ )
1: x+
vi vj = 0 ∀(vi , vj ) ∈ E
2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
4: for s ∈ S do
5:
xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
+
s
6:
x+
vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
+
7:
κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
8: end for
9: Return x+
• Line 1: Initialize x+ as the zero vector.
• Line 2-3: Set κ according to Eq. (12)
• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
2. Line 6: Cut edges in x+ that are cut in xs .
3. Line 7: Set φvi vj to zero for cut edges in x+ .
• Line 9: Return the solution x+
When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],
though we do not exploit its capacity to tackle non-submodular problems.

12

</references>
<discussion></discussion>
<titre></titre> 
<auteur></auteur>
<abstract>Abstract
We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We
reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders
decomposition formulation has many subproblems, each associated with a node in
the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
enforces the cycle inequalities corresponding to edges with negative (repulsive)
weights attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
approaches, theoretically allows for massive parallelization.

</abstract>
<introduction>Introduction

Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
on a sparse graph with real valued edge weights, where nodes correspond to observations and
weighted edges describe the affinity between pairs of nodes.
For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .
The magnitude of the weight is a function of the confidence of the classifier.
The CC cost function sums up the weights of the edges separating connected components (referred
to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
naturally as a function of the edge weights, rather than requiring an additional search over some
model order parameter describing the number of clusters (entities) [37].
Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
programming with cutting planes. They do not scale easily to large CC problem instances and are not
Preprint. Under review.

easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
CC for domains, where massively parallel computation could be employed.
In this paper we apply the classic Benders decomposition from operations research [10] to CC for
computer vision. Benders decomposition is commonly applied in operations research to solve mixed
integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The
block structure requires that no row of the constraint matrix of the MILP contains variables from
more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
proceeds with the master problem solving optimization over its variables. The subsequent solution
of the subproblems can be done in parallel and provides primal/dual solutions over their variables
conditioned on the solution to the master problem. The dual solutions to the subproblems provide
constraints to the master problem. Optimization continues until no further constraints are added to
the master problem.
Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
solving the subproblems not only provides a solution for their variables, but also a lower bound in the
form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
solution to the master problem.
Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
regarding the original objective of the subproblem.
Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].

</introduction> 
<corps>2

Related Work

Correlation clustering has been successfully applied to multiple problems in computer vision including
image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
on random sampling and only provides optimality bounds.
Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
[39], Andres et al. [3].
Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
quality of the solution.
Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
correspond to detections of objects and edges are associated with probabilities of co-association.The
work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
multi-person pose estimation using CC augmented with node labeling.
Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
fractional variables (flows of commodities from facilities to customers in a network) associated with
2

constraints. Benders decomposition reformulates optimization so as to use only the integer variables
and converts the fractional variables into constraints. These constraints are referred to as Benders
rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
use of MWR [23], which are more binding than the standard Benders rows.
Benders decomposition has recently been introduced to computer vision (though not for CC), for the
purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
modeled so as to admit efficient optimization, using column generation and Benders decomposition
jointly. The application of Benders decomposition in our paper is distinct regarding the problem
domain, the underlying integer program and the structure of the Benders subproblems.

3

Standard Correlation Clustering Formulation

In this section, we review the standard optimization formulation for CC [1], which corresponds to a
graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
binary edge labeling problem.
Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
min

x∈{0,1}|E|

s.t.

X
(vi ,vj )∈E −

X

X

−φvi vj (1 − xvi vj ) +

φ vi vj x vi vj

(CC1 )

(vi ,vj )∈E +

xvi vj ≥ xvic vjc

∀c ∈ C,

(1)

(vi ,vj )∈Ec+

where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
(vi , vj ) with xvi vj = 1 as a cut edge.
The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
components. We refer to the constraints in Eq. (1) as cycle inequalities.
Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
ˆ The
corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
cycle inequalities exist, after which the ILP must be solved in each iteration.
We should note that earlier work in CC for computer vision did not require that cycle inequalities
contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
or its LP relaxation.
In this section, we reviewed the baseline approach for solving CC in the computer vision community.
In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
specific solver of Andres et al. [1].
3

4

Benders Decomposition for Correlation Clustering

In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
to denote the subset of E + adjacent to vs .
In this section, we assume that we are provided with S, which can be produced greedily or using an
LP/ILP solver.
Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
number of edges in the subproblem s.
X
X
X
(CC1 )
(CC2 ) :
min
−φvi vj (1 − xvi vj ) +
φ vi vj x vi vj +
Q(φ, s, x),
x∈{0,1}|E|

(vi ,vj )∈E −

(vi ,vj )∈E +

s∈S

(CC2 )
where Q(φ, s, x) is defined as follows.
Q(φ, s, x)

=

min
s

x ∈{0,1}

s.t.

X
|s|

−φvi vj (1 − xsvi vj ) +

(vi ,vj )∈Es−

X

X

φvi vj xsvi vj

(2)

(vi ,vj )∈E +

xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

(vi ,vj )∈Ec+

We now construct a solution x∗ = {x∗vi vj , (xs∗
vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
as follows.
M

x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E + .

(3)
(4)

The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
vi vj and
is defined as follows.

1, if (vi , vj ) ∈ Es−
xs∗
=
(5)
vi vj
0, otherwise.
In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
vi vj )s∈S } is no greater than that of
{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all
s ∈ S.
It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
all s ∈ S.
Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
connected components that are adjacent to one another, not including the root node vs . Note, that
merging any pair of such components, does not increase the cost, since those components are not
separated by negative weight edges in subproblem s and so the result is still a partition.
Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
labeling formulation of min-cut, with the notation below.
4

We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
(
1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
s
f vi vj =
(6)
1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all
(vi , vj ) ∈ Es− .
Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
X
X
Q(φ, s, x) = smin
φvi vj fvsi vj −
φvs v fvss v
(7)
fv v ≥0
i j
(vi ,vj )∈E +
mv ≥0

(vs ,v)∈Es−

λ−
vi vj

:

mvi − mvj ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

λ+
vi vj

:

mvj − mvi ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

ψv−

:

xvs v − fvss v ≤ mv

∀(vs , v) ∈ Es− ,

ψv+

:

mv ≤ xvs v + fvss v

∀(vs , v) ∈ Es+ ,

This yields to the corresponding dual subproblem.
X
+
max −
(λ−
vi vj + λvi vj )xvi vj +
λ≥0
ψ≥0

s.t.

ψv+i

X

ψv− xvs v −

(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

X

ψv+ xvs v

(8)

(vs ,v)∈Es+

1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
X

X

+
(λ−
vi vj − λvi vj ) +

vj
(vi ,vj )∈(E + \Es+ )

φ vi vj

−
(λ+
vj vi − λvj vi ) ≥ 0

∀vi ∈ V − vs

vj
(vj ,vi )∈(E + \Es+ )

−φvs v − ψv− ≥ 0
φvs v − ψv+ ≥ 0
+
− (λ−
vi v j + λ vi vj ) ≥ 0

∀(vs , v) ∈ Es−
∀(vs , v) ∈ Es+
∀(vi , vj ) ∈ (E + \ Es+ ).

In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
xvi vj term.

+
−(λ−
if (vi , vj ) ∈ (E + \ Es+ )

vi vj + λvi vj ),




−ψv+j ,
if (vi , vj ) ∈ Es+
ωvzi vj =

ψv−j ,
if (vi , vj ) ∈ Es−




0,
if (vi , vj ) ∈ (E − \ Es− ).
We denote the set of all dual feasible solutions across s P
∈ S as Z, with z ∈ Z. Observe, that to
enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
We formulate CC as optimization using Z below.
X
X
(CC2 )
(CC3 ) = min
φ vi vj x vi vj −
(1 − xvi vj )φvi vj
(CC3 )
x∈{0,1}|E|

s.t.

X

(vi ,vj )∈E −

(vi ,vj )∈E +

xvi vj ωvzi vj ≤ 0

∀z ∈ Z

(vi ,vj )∈E

5

Algorithm 1 Benders Decomposition for CC (BDCC)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

4.1

Ẑ = {}
done_LP = False
repeat
x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
did_add = False
for s ∈ S do
if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then
z1 = Get Benders row via Eq (8).
z2 = Get MWR via Sec. 5.
Ẑ = Ẑ ∪ z1 ∪ z2
did_add = True
end if
end for
if did_add=False then
done_LP = True
end if
until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
Return x

Cutting Plane Optimization

Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
master problem) and generating new Benders rows until no violated constraints exist.
This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then
we have identified a violated cycle inequality associated with s.
We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
rows, which we describe in the following Sec. 5.
Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
1
provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
vi vj = 1, if xvi vj > 2
∗
and otherwise set x∗∗
vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
∗∗
connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
(supplementary material), we provide a more involved approach to produce feasible integer solutions.
In this section, we characterized CC using Benders decomposition and provided a cutting plane
algorithm to solve the corresponding optimization.

5

Magnanti-Wong Benders Rows

We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
6

Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
and black for not using Magnanti-Wong rows. We show both the computation time with and without
exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
total running time versus the total running time when solving each subproblem is done on its own
CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
where we replace the objective and add one additional constraint.
We follow the tradition of the operations research literature and use a random negative valued vector
(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
−1
subproblem is solved. We experimented with using as an objective .0001+|φ
, which encourages
vi vj |
the cutting of edges with large positive weight, but it works as well as the random negative objective.
Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
X
X
X
+
τ Q(φ, s, x) ≤ −
(λ−
ψv− xvs v −
ψv+ xvs v
vi vj + λvi vj )xvi vj +
(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

(vs ,v)∈Es+

(9)
Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
experiments, we found that τ = 12 provides strong performance.

6

Experiments: Image Segmentation

In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates
optimization.
To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
linear and integer linear programming problems considered during the course of optimization. We use
a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
7

Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
MWR are generated.
=0.1

=1

=10

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.266
0.0426

0.372
0.0532
0.777
0.0745

0.585
0.0745
0.904
0.0745

0.894
0.106
0.968
0.138

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.319
0.0532

0.394
0.0638
0.819
0.0745

0.606
0.0745
0.947
0.106

0.904
0.16
0.979
0.17

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.202
0.0532
0.447
0.0638

0.426
0.0957
0.936
0.128

0.628
0.128
0.979
0.181

0.915
0.223
0.989
0.287

We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
minimum vertex cover problem can be solved approximately or greedily.
In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
the exact value of τ does not effect the speed of optimization dramatically. We show performance
with and without relying on parallel processing. Our parallel processing times assume that we
have one CPU for each subproblem. For the problem instances in our application the number of
subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits
of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
improvement, since the master problem consumes a larger proportion of total CPU time.
In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
instances, the total CPU time required when using no MWR was prohibitively large, which is not the
case when MWR are employed. Thus most problem instances solved without MWR terminated early.
In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
the difference between the anytime solution (upper bound) and the lower bound on the objective. For
each such tolerance , we compute the percentage of instances, for which the duality gap is less than
, after various amounts of time. We observe that the performance of optimization without MWR,
but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
demonstrates that, across the dataset, MWR are of greater importance than parallelization.

7

</corps> 
<conclusion>Conclusions

We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
expect will find further use in the application of combinatorial optimization to problems in computer
vision.
8

The exploitation of results from the domain of operations research may lead to improved variants
of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
problem so as to enforce integrality on a subset of the variables of the master problem.

</conclusion>
<acknowledgement></acknowledgement> 
<references>References
[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
(ICCV-11), pages 2611–2618, 2011.
[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
International Conference on Computer Vision (ECCV-12), 2012.
[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting
planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
238–247, 2002.
[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
solver for multicut partitioning. In CVPR, 2014.
[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
10.1007/978-3-319-46475-6_44.
[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.
[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations
research, 33(5):989–1007, 1985.
[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft
routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
1776–1781, 1966.
[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
399–404, 1956.
[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
Management science, 20(5):822–844, 1974.
[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
Research (volume 9), 1961.
[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
Springer, 2016.
[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
2015.

9

[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
image and mesh graphs by lifted multicuts. In ICCV, 2015.
[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.
In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
Mathematical Programming Computation, 1(1):43–67, 2009.
[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
model selection criteria. Operations research, 29(3):464–484, 1981.
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 769–776. ACM, 2009.
[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2969239.2969249.
[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:
Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
june 2007.
[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
IEEE Transactions on, 39(5):694–697, May 1990.
[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
CVPR, 2017.
[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
CVPR, 2015.
[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 652–666, 2018.
[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
Systems, 2015.
[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
preprint arXiv:1805.04958, 2018.
[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.
In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
microscopy. In ISBI, 2014.

10

A

APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.
Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
vi vj )s∈S } is constructed, for which
Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of
xs .
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E +

M

x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
M

∀(vi , vj ) ∈ E +

M

∀(vi , vj ) ∈ Es− , s ∈ S.

xs∗
vi vj = 0
xs∗
vi vj = 1

(10)

The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the
optimizing solution for f in subproblem s, given x, x∗ respectively.
x∗vi vj = xvi vj + max fvsi vj
s∈S

x∗vi vj = xvi vj − fvsi vj

∀(vi , vj ) ∈ E +

∀(vi , vj ) ∈ Es− , s ∈ S

fvs∗
= 0 ∀(vi , vj ) ∈ E +
i vj

(11)

fvs∗
= 0 ∀(vi , vj ) ∈ Es−
i vj
These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that
since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),
which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total
P
decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the
former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand
the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields
in an increase of the objective of the master problem by −φvi vj (1 − xn
vi vj ), while the objective of subproblem
s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

B

Line by Line Description of BDCC

We provide the line by line description of Alg. 1.
• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
• Line 2: Indicate that we have not solved the LP relaxation yet.
• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
integral solution is produced.
1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
indicated by done_lp=True.
2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.
– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
set Ẑ.
– Line 11: Indicate that a Benders row was added this iteration.
4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
solving the master problem for the remainder of the algorithm.
• Line 18 Return solution x.

11

C

Generating Feasible Integer Solutions Prior to Convergence

Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so
that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and
the relaxation is small. In this section we consider the production of feasible integer solutions, given the current
solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this
procedure as rounding.
Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
using x∗ below.
κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
κvi vj =

φvi vj x∗vi vj

∀(vi , vj ) ∈ E

(12)

−

∗

Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let
xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
vi vj = 1 if exactly
one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where
s∗
x0s
as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
vi vj = 1Es− (vi , vj ), is achieved using x
s∗
The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
then the solution produced below has cost equal to that of x∗ .
M

xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
M

s∗
x+
vi vj = max xvi vj
s∈S

M

s∗
x+
vi vj = xvi vj

∀(vi , vj ) ∈ E +

(13)

∀(vi , vj ) ∈ Es− , s ∈ S

The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to
integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.
We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+
by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of
edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
x∗ )
1: x+
vi vj = 0 ∀(vi , vj ) ∈ E
2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
4: for s ∈ S do
5:
xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
+
s
6:
x+
vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
+
7:
κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
8: end for
9: Return x+
• Line 1: Initialize x+ as the zero vector.
• Line 2-3: Set κ according to Eq. (12)
• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
2. Line 6: Cut edges in x+ that are cut in xs .
3. Line 7: Set φvi vj to zero for cut edges in x+ .
• Line 9: Return the solution x+
When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],
though we do not exploit its capacity to tackle non-submodular problems.

12

</references>
<discussion></discussion>
<titre></titre> 
<auteur></auteur>
<abstract>Abstract
We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We
reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders
decomposition formulation has many subproblems, each associated with a node in
the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
enforces the cycle inequalities corresponding to edges with negative (repulsive)
weights attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
approaches, theoretically allows for massive parallelization.

</abstract>
<introduction>Introduction

Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
on a sparse graph with real valued edge weights, where nodes correspond to observations and
weighted edges describe the affinity between pairs of nodes.
For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .
The magnitude of the weight is a function of the confidence of the classifier.
The CC cost function sums up the weights of the edges separating connected components (referred
to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
naturally as a function of the edge weights, rather than requiring an additional search over some
model order parameter describing the number of clusters (entities) [37].
Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
programming with cutting planes. They do not scale easily to large CC problem instances and are not
Preprint. Under review.

easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
CC for domains, where massively parallel computation could be employed.
In this paper we apply the classic Benders decomposition from operations research [10] to CC for
computer vision. Benders decomposition is commonly applied in operations research to solve mixed
integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The
block structure requires that no row of the constraint matrix of the MILP contains variables from
more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
proceeds with the master problem solving optimization over its variables. The subsequent solution
of the subproblems can be done in parallel and provides primal/dual solutions over their variables
conditioned on the solution to the master problem. The dual solutions to the subproblems provide
constraints to the master problem. Optimization continues until no further constraints are added to
the master problem.
Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
solving the subproblems not only provides a solution for their variables, but also a lower bound in the
form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
solution to the master problem.
Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
regarding the original objective of the subproblem.
Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].

</introduction> 
<corps>2

Related Work

Correlation clustering has been successfully applied to multiple problems in computer vision including
image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
on random sampling and only provides optimality bounds.
Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
[39], Andres et al. [3].
Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
quality of the solution.
Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
correspond to detections of objects and edges are associated with probabilities of co-association.The
work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
multi-person pose estimation using CC augmented with node labeling.
Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
fractional variables (flows of commodities from facilities to customers in a network) associated with
2

constraints. Benders decomposition reformulates optimization so as to use only the integer variables
and converts the fractional variables into constraints. These constraints are referred to as Benders
rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
use of MWR [23], which are more binding than the standard Benders rows.
Benders decomposition has recently been introduced to computer vision (though not for CC), for the
purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
modeled so as to admit efficient optimization, using column generation and Benders decomposition
jointly. The application of Benders decomposition in our paper is distinct regarding the problem
domain, the underlying integer program and the structure of the Benders subproblems.

3

Standard Correlation Clustering Formulation

In this section, we review the standard optimization formulation for CC [1], which corresponds to a
graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
binary edge labeling problem.
Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
min

x∈{0,1}|E|

s.t.

X
(vi ,vj )∈E −

X

X

−φvi vj (1 − xvi vj ) +

φ vi vj x vi vj

(CC1 )

(vi ,vj )∈E +

xvi vj ≥ xvic vjc

∀c ∈ C,

(1)

(vi ,vj )∈Ec+

where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
(vi , vj ) with xvi vj = 1 as a cut edge.
The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
components. We refer to the constraints in Eq. (1) as cycle inequalities.
Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
ˆ The
corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
cycle inequalities exist, after which the ILP must be solved in each iteration.
We should note that earlier work in CC for computer vision did not require that cycle inequalities
contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
or its LP relaxation.
In this section, we reviewed the baseline approach for solving CC in the computer vision community.
In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
specific solver of Andres et al. [1].
3

4

Benders Decomposition for Correlation Clustering

In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
to denote the subset of E + adjacent to vs .
In this section, we assume that we are provided with S, which can be produced greedily or using an
LP/ILP solver.
Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
number of edges in the subproblem s.
X
X
X
(CC1 )
(CC2 ) :
min
−φvi vj (1 − xvi vj ) +
φ vi vj x vi vj +
Q(φ, s, x),
x∈{0,1}|E|

(vi ,vj )∈E −

(vi ,vj )∈E +

s∈S

(CC2 )
where Q(φ, s, x) is defined as follows.
Q(φ, s, x)

=

min
s

x ∈{0,1}

s.t.

X
|s|

−φvi vj (1 − xsvi vj ) +

(vi ,vj )∈Es−

X

X

φvi vj xsvi vj

(2)

(vi ,vj )∈E +

xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

(vi ,vj )∈Ec+

We now construct a solution x∗ = {x∗vi vj , (xs∗
vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
as follows.
M

x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E + .

(3)
(4)

The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
vi vj and
is defined as follows.

1, if (vi , vj ) ∈ Es−
xs∗
=
(5)
vi vj
0, otherwise.
In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
vi vj )s∈S } is no greater than that of
{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all
s ∈ S.
It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
all s ∈ S.
Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
connected components that are adjacent to one another, not including the root node vs . Note, that
merging any pair of such components, does not increase the cost, since those components are not
separated by negative weight edges in subproblem s and so the result is still a partition.
Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
labeling formulation of min-cut, with the notation below.
4

We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
(
1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
s
f vi vj =
(6)
1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all
(vi , vj ) ∈ Es− .
Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
X
X
Q(φ, s, x) = smin
φvi vj fvsi vj −
φvs v fvss v
(7)
fv v ≥0
i j
(vi ,vj )∈E +
mv ≥0

(vs ,v)∈Es−

λ−
vi vj

:

mvi − mvj ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

λ+
vi vj

:

mvj − mvi ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

ψv−

:

xvs v − fvss v ≤ mv

∀(vs , v) ∈ Es− ,

ψv+

:

mv ≤ xvs v + fvss v

∀(vs , v) ∈ Es+ ,

This yields to the corresponding dual subproblem.
X
+
max −
(λ−
vi vj + λvi vj )xvi vj +
λ≥0
ψ≥0

s.t.

ψv+i

X

ψv− xvs v −

(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

X

ψv+ xvs v

(8)

(vs ,v)∈Es+

1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
X

X

+
(λ−
vi vj − λvi vj ) +

vj
(vi ,vj )∈(E + \Es+ )

φ vi vj

−
(λ+
vj vi − λvj vi ) ≥ 0

∀vi ∈ V − vs

vj
(vj ,vi )∈(E + \Es+ )

−φvs v − ψv− ≥ 0
φvs v − ψv+ ≥ 0
+
− (λ−
vi v j + λ vi vj ) ≥ 0

∀(vs , v) ∈ Es−
∀(vs , v) ∈ Es+
∀(vi , vj ) ∈ (E + \ Es+ ).

In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
xvi vj term.

+
−(λ−
if (vi , vj ) ∈ (E + \ Es+ )

vi vj + λvi vj ),




−ψv+j ,
if (vi , vj ) ∈ Es+
ωvzi vj =

ψv−j ,
if (vi , vj ) ∈ Es−




0,
if (vi , vj ) ∈ (E − \ Es− ).
We denote the set of all dual feasible solutions across s P
∈ S as Z, with z ∈ Z. Observe, that to
enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
We formulate CC as optimization using Z below.
X
X
(CC2 )
(CC3 ) = min
φ vi vj x vi vj −
(1 − xvi vj )φvi vj
(CC3 )
x∈{0,1}|E|

s.t.

X

(vi ,vj )∈E −

(vi ,vj )∈E +

xvi vj ωvzi vj ≤ 0

∀z ∈ Z

(vi ,vj )∈E

5

Algorithm 1 Benders Decomposition for CC (BDCC)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

4.1

Ẑ = {}
done_LP = False
repeat
x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
did_add = False
for s ∈ S do
if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then
z1 = Get Benders row via Eq (8).
z2 = Get MWR via Sec. 5.
Ẑ = Ẑ ∪ z1 ∪ z2
did_add = True
end if
end for
if did_add=False then
done_LP = True
end if
until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
Return x

Cutting Plane Optimization

Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
master problem) and generating new Benders rows until no violated constraints exist.
This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then
we have identified a violated cycle inequality associated with s.
We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
rows, which we describe in the following Sec. 5.
Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
1
provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
vi vj = 1, if xvi vj > 2
∗
and otherwise set x∗∗
vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
∗∗
connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
(supplementary material), we provide a more involved approach to produce feasible integer solutions.
In this section, we characterized CC using Benders decomposition and provided a cutting plane
algorithm to solve the corresponding optimization.

5

Magnanti-Wong Benders Rows

We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
6

Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
and black for not using Magnanti-Wong rows. We show both the computation time with and without
exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
total running time versus the total running time when solving each subproblem is done on its own
CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
where we replace the objective and add one additional constraint.
We follow the tradition of the operations research literature and use a random negative valued vector
(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
−1
subproblem is solved. We experimented with using as an objective .0001+|φ
, which encourages
vi vj |
the cutting of edges with large positive weight, but it works as well as the random negative objective.
Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
X
X
X
+
τ Q(φ, s, x) ≤ −
(λ−
ψv− xvs v −
ψv+ xvs v
vi vj + λvi vj )xvi vj +
(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

(vs ,v)∈Es+

(9)
Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
experiments, we found that τ = 12 provides strong performance.

6

Experiments: Image Segmentation

In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates
optimization.
To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
linear and integer linear programming problems considered during the course of optimization. We use
a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
7

Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
MWR are generated.
=0.1

=1

=10

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.266
0.0426

0.372
0.0532
0.777
0.0745

0.585
0.0745
0.904
0.0745

0.894
0.106
0.968
0.138

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.319
0.0532

0.394
0.0638
0.819
0.0745

0.606
0.0745
0.947
0.106

0.904
0.16
0.979
0.17

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.202
0.0532
0.447
0.0638

0.426
0.0957
0.936
0.128

0.628
0.128
0.979
0.181

0.915
0.223
0.989
0.287

We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
minimum vertex cover problem can be solved approximately or greedily.
In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
the exact value of τ does not effect the speed of optimization dramatically. We show performance
with and without relying on parallel processing. Our parallel processing times assume that we
have one CPU for each subproblem. For the problem instances in our application the number of
subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits
of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
improvement, since the master problem consumes a larger proportion of total CPU time.
In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
instances, the total CPU time required when using no MWR was prohibitively large, which is not the
case when MWR are employed. Thus most problem instances solved without MWR terminated early.
In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
the difference between the anytime solution (upper bound) and the lower bound on the objective. For
each such tolerance , we compute the percentage of instances, for which the duality gap is less than
, after various amounts of time. We observe that the performance of optimization without MWR,
but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
demonstrates that, across the dataset, MWR are of greater importance than parallelization.

7

</corps> 
<conclusion>Conclusions

We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
expect will find further use in the application of combinatorial optimization to problems in computer
vision.
8

The exploitation of results from the domain of operations research may lead to improved variants
of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
problem so as to enforce integrality on a subset of the variables of the master problem.

</conclusion>
<acknowledgement></acknowledgement> 
<references>References
[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
(ICCV-11), pages 2611–2618, 2011.
[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
International Conference on Computer Vision (ECCV-12), 2012.
[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting
planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
238–247, 2002.
[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
solver for multicut partitioning. In CVPR, 2014.
[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
10.1007/978-3-319-46475-6_44.
[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.
[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations
research, 33(5):989–1007, 1985.
[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft
routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
1776–1781, 1966.
[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
399–404, 1956.
[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
Management science, 20(5):822–844, 1974.
[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
Research (volume 9), 1961.
[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
Springer, 2016.
[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
2015.

9

[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
image and mesh graphs by lifted multicuts. In ICCV, 2015.
[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.
In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
Mathematical Programming Computation, 1(1):43–67, 2009.
[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
model selection criteria. Operations research, 29(3):464–484, 1981.
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 769–776. ACM, 2009.
[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2969239.2969249.
[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:
Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
june 2007.
[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
IEEE Transactions on, 39(5):694–697, May 1990.
[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
CVPR, 2017.
[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
CVPR, 2015.
[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 652–666, 2018.
[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
Systems, 2015.
[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
preprint arXiv:1805.04958, 2018.
[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.
In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
microscopy. In ISBI, 2014.

10

A

APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.
Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
vi vj )s∈S } is constructed, for which
Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of
xs .
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E +

M

x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
M

∀(vi , vj ) ∈ E +

M

∀(vi , vj ) ∈ Es− , s ∈ S.

xs∗
vi vj = 0
xs∗
vi vj = 1

(10)

The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the
optimizing solution for f in subproblem s, given x, x∗ respectively.
x∗vi vj = xvi vj + max fvsi vj
s∈S

x∗vi vj = xvi vj − fvsi vj

∀(vi , vj ) ∈ E +

∀(vi , vj ) ∈ Es− , s ∈ S

fvs∗
= 0 ∀(vi , vj ) ∈ E +
i vj

(11)

fvs∗
= 0 ∀(vi , vj ) ∈ Es−
i vj
These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that
since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),
which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total
P
decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the
former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand
the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields
in an increase of the objective of the master problem by −φvi vj (1 − xn
vi vj ), while the objective of subproblem
s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

B

Line by Line Description of BDCC

We provide the line by line description of Alg. 1.
• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
• Line 2: Indicate that we have not solved the LP relaxation yet.
• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
integral solution is produced.
1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
indicated by done_lp=True.
2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.
– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
set Ẑ.
– Line 11: Indicate that a Benders row was added this iteration.
4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
solving the master problem for the remainder of the algorithm.
• Line 18 Return solution x.

11

C

Generating Feasible Integer Solutions Prior to Convergence

Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so
that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and
the relaxation is small. In this section we consider the production of feasible integer solutions, given the current
solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this
procedure as rounding.
Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
using x∗ below.
κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
κvi vj =

φvi vj x∗vi vj

∀(vi , vj ) ∈ E

(12)

−

∗

Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let
xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
vi vj = 1 if exactly
one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where
s∗
x0s
as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
vi vj = 1Es− (vi , vj ), is achieved using x
s∗
The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
then the solution produced below has cost equal to that of x∗ .
M

xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
M

s∗
x+
vi vj = max xvi vj
s∈S

M

s∗
x+
vi vj = xvi vj

∀(vi , vj ) ∈ E +

(13)

∀(vi , vj ) ∈ Es− , s ∈ S

The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to
integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.
We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+
by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of
edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
x∗ )
1: x+
vi vj = 0 ∀(vi , vj ) ∈ E
2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
4: for s ∈ S do
5:
xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
+
s
6:
x+
vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
+
7:
κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
8: end for
9: Return x+
• Line 1: Initialize x+ as the zero vector.
• Line 2-3: Set κ according to Eq. (12)
• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
2. Line 6: Cut edges in x+ that are cut in xs .
3. Line 7: Set φvi vj to zero for cut edges in x+ .
• Line 9: Return the solution x+
When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],
though we do not exploit its capacity to tackle non-submodular problems.

12

</references>
<discussion></discussion>
<titre></titre> 
<auteur></auteur>
<abstract>Abstract
We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We
reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders
decomposition formulation has many subproblems, each associated with a node in
the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
enforces the cycle inequalities corresponding to edges with negative (repulsive)
weights attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
approaches, theoretically allows for massive parallelization.

</abstract>
<introduction>Introduction

Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
on a sparse graph with real valued edge weights, where nodes correspond to observations and
weighted edges describe the affinity between pairs of nodes.
For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .
The magnitude of the weight is a function of the confidence of the classifier.
The CC cost function sums up the weights of the edges separating connected components (referred
to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
naturally as a function of the edge weights, rather than requiring an additional search over some
model order parameter describing the number of clusters (entities) [37].
Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
programming with cutting planes. They do not scale easily to large CC problem instances and are not
Preprint. Under review.

easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
CC for domains, where massively parallel computation could be employed.
In this paper we apply the classic Benders decomposition from operations research [10] to CC for
computer vision. Benders decomposition is commonly applied in operations research to solve mixed
integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The
block structure requires that no row of the constraint matrix of the MILP contains variables from
more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
proceeds with the master problem solving optimization over its variables. The subsequent solution
of the subproblems can be done in parallel and provides primal/dual solutions over their variables
conditioned on the solution to the master problem. The dual solutions to the subproblems provide
constraints to the master problem. Optimization continues until no further constraints are added to
the master problem.
Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
solving the subproblems not only provides a solution for their variables, but also a lower bound in the
form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
solution to the master problem.
Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
regarding the original objective of the subproblem.
Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].

</introduction> 
<corps>2

Related Work

Correlation clustering has been successfully applied to multiple problems in computer vision including
image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
on random sampling and only provides optimality bounds.
Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
[39], Andres et al. [3].
Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
quality of the solution.
Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
correspond to detections of objects and edges are associated with probabilities of co-association.The
work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
multi-person pose estimation using CC augmented with node labeling.
Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
fractional variables (flows of commodities from facilities to customers in a network) associated with
2

constraints. Benders decomposition reformulates optimization so as to use only the integer variables
and converts the fractional variables into constraints. These constraints are referred to as Benders
rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
use of MWR [23], which are more binding than the standard Benders rows.
Benders decomposition has recently been introduced to computer vision (though not for CC), for the
purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
modeled so as to admit efficient optimization, using column generation and Benders decomposition
jointly. The application of Benders decomposition in our paper is distinct regarding the problem
domain, the underlying integer program and the structure of the Benders subproblems.

3

Standard Correlation Clustering Formulation

In this section, we review the standard optimization formulation for CC [1], which corresponds to a
graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
binary edge labeling problem.
Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
min

x∈{0,1}|E|

s.t.

X
(vi ,vj )∈E −

X

X

−φvi vj (1 − xvi vj ) +

φ vi vj x vi vj

(CC1 )

(vi ,vj )∈E +

xvi vj ≥ xvic vjc

∀c ∈ C,

(1)

(vi ,vj )∈Ec+

where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
(vi , vj ) with xvi vj = 1 as a cut edge.
The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
components. We refer to the constraints in Eq. (1) as cycle inequalities.
Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
ˆ The
corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
cycle inequalities exist, after which the ILP must be solved in each iteration.
We should note that earlier work in CC for computer vision did not require that cycle inequalities
contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
or its LP relaxation.
In this section, we reviewed the baseline approach for solving CC in the computer vision community.
In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
specific solver of Andres et al. [1].
3

4

Benders Decomposition for Correlation Clustering

In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
to denote the subset of E + adjacent to vs .
In this section, we assume that we are provided with S, which can be produced greedily or using an
LP/ILP solver.
Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
number of edges in the subproblem s.
X
X
X
(CC1 )
(CC2 ) :
min
−φvi vj (1 − xvi vj ) +
φ vi vj x vi vj +
Q(φ, s, x),
x∈{0,1}|E|

(vi ,vj )∈E −

(vi ,vj )∈E +

s∈S

(CC2 )
where Q(φ, s, x) is defined as follows.
Q(φ, s, x)

=

min
s

x ∈{0,1}

s.t.

X
|s|

−φvi vj (1 − xsvi vj ) +

(vi ,vj )∈Es−

X

X

φvi vj xsvi vj

(2)

(vi ,vj )∈E +

xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

(vi ,vj )∈Ec+

We now construct a solution x∗ = {x∗vi vj , (xs∗
vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
as follows.
M

x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E + .

(3)
(4)

The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
vi vj and
is defined as follows.

1, if (vi , vj ) ∈ Es−
xs∗
=
(5)
vi vj
0, otherwise.
In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
vi vj )s∈S } is no greater than that of
{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all
s ∈ S.
It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
all s ∈ S.
Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
connected components that are adjacent to one another, not including the root node vs . Note, that
merging any pair of such components, does not increase the cost, since those components are not
separated by negative weight edges in subproblem s and so the result is still a partition.
Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
labeling formulation of min-cut, with the notation below.
4

We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
(
1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
s
f vi vj =
(6)
1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all
(vi , vj ) ∈ Es− .
Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
X
X
Q(φ, s, x) = smin
φvi vj fvsi vj −
φvs v fvss v
(7)
fv v ≥0
i j
(vi ,vj )∈E +
mv ≥0

(vs ,v)∈Es−

λ−
vi vj

:

mvi − mvj ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

λ+
vi vj

:

mvj − mvi ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

ψv−

:

xvs v − fvss v ≤ mv

∀(vs , v) ∈ Es− ,

ψv+

:

mv ≤ xvs v + fvss v

∀(vs , v) ∈ Es+ ,

This yields to the corresponding dual subproblem.
X
+
max −
(λ−
vi vj + λvi vj )xvi vj +
λ≥0
ψ≥0

s.t.

ψv+i

X

ψv− xvs v −

(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

X

ψv+ xvs v

(8)

(vs ,v)∈Es+

1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
X

X

+
(λ−
vi vj − λvi vj ) +

vj
(vi ,vj )∈(E + \Es+ )

φ vi vj

−
(λ+
vj vi − λvj vi ) ≥ 0

∀vi ∈ V − vs

vj
(vj ,vi )∈(E + \Es+ )

−φvs v − ψv− ≥ 0
φvs v − ψv+ ≥ 0
+
− (λ−
vi v j + λ vi vj ) ≥ 0

∀(vs , v) ∈ Es−
∀(vs , v) ∈ Es+
∀(vi , vj ) ∈ (E + \ Es+ ).

In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
xvi vj term.

+
−(λ−
if (vi , vj ) ∈ (E + \ Es+ )

vi vj + λvi vj ),




−ψv+j ,
if (vi , vj ) ∈ Es+
ωvzi vj =

ψv−j ,
if (vi , vj ) ∈ Es−




0,
if (vi , vj ) ∈ (E − \ Es− ).
We denote the set of all dual feasible solutions across s P
∈ S as Z, with z ∈ Z. Observe, that to
enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
We formulate CC as optimization using Z below.
X
X
(CC2 )
(CC3 ) = min
φ vi vj x vi vj −
(1 − xvi vj )φvi vj
(CC3 )
x∈{0,1}|E|

s.t.

X

(vi ,vj )∈E −

(vi ,vj )∈E +

xvi vj ωvzi vj ≤ 0

∀z ∈ Z

(vi ,vj )∈E

5

Algorithm 1 Benders Decomposition for CC (BDCC)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

4.1

Ẑ = {}
done_LP = False
repeat
x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
did_add = False
for s ∈ S do
if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then
z1 = Get Benders row via Eq (8).
z2 = Get MWR via Sec. 5.
Ẑ = Ẑ ∪ z1 ∪ z2
did_add = True
end if
end for
if did_add=False then
done_LP = True
end if
until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
Return x

Cutting Plane Optimization

Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
master problem) and generating new Benders rows until no violated constraints exist.
This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then
we have identified a violated cycle inequality associated with s.
We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
rows, which we describe in the following Sec. 5.
Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
1
provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
vi vj = 1, if xvi vj > 2
∗
and otherwise set x∗∗
vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
∗∗
connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
(supplementary material), we provide a more involved approach to produce feasible integer solutions.
In this section, we characterized CC using Benders decomposition and provided a cutting plane
algorithm to solve the corresponding optimization.

5

Magnanti-Wong Benders Rows

We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
6

Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
and black for not using Magnanti-Wong rows. We show both the computation time with and without
exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
total running time versus the total running time when solving each subproblem is done on its own
CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
where we replace the objective and add one additional constraint.
We follow the tradition of the operations research literature and use a random negative valued vector
(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
−1
subproblem is solved. We experimented with using as an objective .0001+|φ
, which encourages
vi vj |
the cutting of edges with large positive weight, but it works as well as the random negative objective.
Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
X
X
X
+
τ Q(φ, s, x) ≤ −
(λ−
ψv− xvs v −
ψv+ xvs v
vi vj + λvi vj )xvi vj +
(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

(vs ,v)∈Es+

(9)
Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
experiments, we found that τ = 12 provides strong performance.

6

Experiments: Image Segmentation

In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates
optimization.
To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
linear and integer linear programming problems considered during the course of optimization. We use
a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
7

Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
MWR are generated.
=0.1

=1

=10

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.266
0.0426

0.372
0.0532
0.777
0.0745

0.585
0.0745
0.904
0.0745

0.894
0.106
0.968
0.138

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.319
0.0532

0.394
0.0638
0.819
0.0745

0.606
0.0745
0.947
0.106

0.904
0.16
0.979
0.17

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.202
0.0532
0.447
0.0638

0.426
0.0957
0.936
0.128

0.628
0.128
0.979
0.181

0.915
0.223
0.989
0.287

We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
minimum vertex cover problem can be solved approximately or greedily.
In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
the exact value of τ does not effect the speed of optimization dramatically. We show performance
with and without relying on parallel processing. Our parallel processing times assume that we
have one CPU for each subproblem. For the problem instances in our application the number of
subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits
of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
improvement, since the master problem consumes a larger proportion of total CPU time.
In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
instances, the total CPU time required when using no MWR was prohibitively large, which is not the
case when MWR are employed. Thus most problem instances solved without MWR terminated early.
In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
the difference between the anytime solution (upper bound) and the lower bound on the objective. For
each such tolerance , we compute the percentage of instances, for which the duality gap is less than
, after various amounts of time. We observe that the performance of optimization without MWR,
but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
demonstrates that, across the dataset, MWR are of greater importance than parallelization.

7

</corps> 
<conclusion>Conclusions

We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
expect will find further use in the application of combinatorial optimization to problems in computer
vision.
8

The exploitation of results from the domain of operations research may lead to improved variants
of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
problem so as to enforce integrality on a subset of the variables of the master problem.

</conclusion>
<acknowledgement></acknowledgement> 
<references>References
[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
(ICCV-11), pages 2611–2618, 2011.
[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
International Conference on Computer Vision (ECCV-12), 2012.
[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting
planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
238–247, 2002.
[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
solver for multicut partitioning. In CVPR, 2014.
[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
10.1007/978-3-319-46475-6_44.
[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.
[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations
research, 33(5):989–1007, 1985.
[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft
routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
1776–1781, 1966.
[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
399–404, 1956.
[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
Management science, 20(5):822–844, 1974.
[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
Research (volume 9), 1961.
[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
Springer, 2016.
[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
2015.

9

[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
image and mesh graphs by lifted multicuts. In ICCV, 2015.
[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.
In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
Mathematical Programming Computation, 1(1):43–67, 2009.
[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
model selection criteria. Operations research, 29(3):464–484, 1981.
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 769–776. ACM, 2009.
[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2969239.2969249.
[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:
Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
june 2007.
[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
IEEE Transactions on, 39(5):694–697, May 1990.
[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
CVPR, 2017.
[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
CVPR, 2015.
[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 652–666, 2018.
[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
Systems, 2015.
[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
preprint arXiv:1805.04958, 2018.
[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.
In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
microscopy. In ISBI, 2014.

10

A

APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.
Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
vi vj )s∈S } is constructed, for which
Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of
xs .
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E +

M

x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
M

∀(vi , vj ) ∈ E +

M

∀(vi , vj ) ∈ Es− , s ∈ S.

xs∗
vi vj = 0
xs∗
vi vj = 1

(10)

The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the
optimizing solution for f in subproblem s, given x, x∗ respectively.
x∗vi vj = xvi vj + max fvsi vj
s∈S

x∗vi vj = xvi vj − fvsi vj

∀(vi , vj ) ∈ E +

∀(vi , vj ) ∈ Es− , s ∈ S

fvs∗
= 0 ∀(vi , vj ) ∈ E +
i vj

(11)

fvs∗
= 0 ∀(vi , vj ) ∈ Es−
i vj
These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that
since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),
which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total
P
decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the
former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand
the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields
in an increase of the objective of the master problem by −φvi vj (1 − xn
vi vj ), while the objective of subproblem
s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

B

Line by Line Description of BDCC

We provide the line by line description of Alg. 1.
• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
• Line 2: Indicate that we have not solved the LP relaxation yet.
• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
integral solution is produced.
1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
indicated by done_lp=True.
2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.
– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
set Ẑ.
– Line 11: Indicate that a Benders row was added this iteration.
4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
solving the master problem for the remainder of the algorithm.
• Line 18 Return solution x.

11

C

Generating Feasible Integer Solutions Prior to Convergence

Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so
that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and
the relaxation is small. In this section we consider the production of feasible integer solutions, given the current
solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this
procedure as rounding.
Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
using x∗ below.
κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
κvi vj =

φvi vj x∗vi vj

∀(vi , vj ) ∈ E

(12)

−

∗

Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let
xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
vi vj = 1 if exactly
one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where
s∗
x0s
as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
vi vj = 1Es− (vi , vj ), is achieved using x
s∗
The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
then the solution produced below has cost equal to that of x∗ .
M

xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
M

s∗
x+
vi vj = max xvi vj
s∈S

M

s∗
x+
vi vj = xvi vj

∀(vi , vj ) ∈ E +

(13)

∀(vi , vj ) ∈ Es− , s ∈ S

The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to
integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.
We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+
by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of
edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
x∗ )
1: x+
vi vj = 0 ∀(vi , vj ) ∈ E
2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
4: for s ∈ S do
5:
xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
+
s
6:
x+
vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
+
7:
κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
8: end for
9: Return x+
• Line 1: Initialize x+ as the zero vector.
• Line 2-3: Set κ according to Eq. (12)
• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
2. Line 6: Cut edges in x+ that are cut in xs .
3. Line 7: Set φvi vj to zero for cut edges in x+ .
• Line 9: Return the solution x+
When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],
though we do not exploit its capacity to tackle non-submodular problems.

12

</references>
<discussion></discussion>
<titre></titre> 
<auteur></auteur>
<abstract>Abstract
We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We
reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders
decomposition formulation has many subproblems, each associated with a node in
the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
enforces the cycle inequalities corresponding to edges with negative (repulsive)
weights attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
approaches, theoretically allows for massive parallelization.

</abstract>
<introduction>Introduction

Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
on a sparse graph with real valued edge weights, where nodes correspond to observations and
weighted edges describe the affinity between pairs of nodes.
For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than 12 .
The magnitude of the weight is a function of the confidence of the classifier.
The CC cost function sums up the weights of the edges separating connected components (referred
to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
naturally as a function of the edge weights, rather than requiring an additional search over some
model order parameter describing the number of clusters (entities) [37].
Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
programming with cutting planes. They do not scale easily to large CC problem instances and are not
Preprint. Under review.

easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
CC for domains, where massively parallel computation could be employed.
In this paper we apply the classic Benders decomposition from operations research [10] to CC for
computer vision. Benders decomposition is commonly applied in operations research to solve mixed
integer linear programs (MILP) that have a special but common block structure. Benders decomposition partitions the variables in the MILP between a master problem and a set of subproblems. The
block structure requires that no row of the constraint matrix of the MILP contains variables from
more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
proceeds with the master problem solving optimization over its variables. The subsequent solution
of the subproblems can be done in parallel and provides primal/dual solutions over their variables
conditioned on the solution to the master problem. The dual solutions to the subproblems provide
constraints to the master problem. Optimization continues until no further constraints are added to
the master problem.
Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
solving the subproblems not only provides a solution for their variables, but also a lower bound in the
form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
solution to the master problem.
Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
regarding the original objective of the subproblem.
Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].

</introduction> 
<corps>2

Related Work

Correlation clustering has been successfully applied to multiple problems in computer vision including
image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
on random sampling and only provides optimality bounds.
Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
[39], Andres et al. [3].
Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
quality of the solution.
Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
correspond to detections of objects and edges are associated with probabilities of co-association.The
work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
multi-person pose estimation using CC augmented with node labeling.
Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
fractional variables (flows of commodities from facilities to customers in a network) associated with
2

constraints. Benders decomposition reformulates optimization so as to use only the integer variables
and converts the fractional variables into constraints. These constraints are referred to as Benders
rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
use of MWR [23], which are more binding than the standard Benders rows.
Benders decomposition has recently been introduced to computer vision (though not for CC), for the
purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
modeled so as to admit efficient optimization, using column generation and Benders decomposition
jointly. The application of Benders decomposition in our paper is distinct regarding the problem
domain, the underlying integer program and the structure of the Benders subproblems.

3

Standard Correlation Clustering Formulation

In this section, we review the standard optimization formulation for CC [1], which corresponds to a
graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
binary edge labeling problem.
Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
min

x∈{0,1}|E|

s.t.

X
(vi ,vj )∈E −

X

X

−φvi vj (1 − xvi vj ) +

φ vi vj x vi vj

(CC1 )

(vi ,vj )∈E +

xvi vj ≥ xvic vjc

∀c ∈ C,

(1)

(vi ,vj )∈Ec+

where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
(vi , vj ) with xvi vj = 1 as a cut edge.
The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
components. We refer to the constraints in Eq. (1) as cycle inequalities.
Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
(initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
ˆ The
corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
cycle inequalities exist, after which the ILP must be solved in each iteration.
We should note that earlier work in CC for computer vision did not require that cycle inequalities
contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
or its LP relaxation.
In this section, we reviewed the baseline approach for solving CC in the computer vision community.
In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
specific solver of Andres et al. [1].
3

4

Benders Decomposition for Correlation Clustering

In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
to denote the subset of E + adjacent to vs .
In this section, we assume that we are provided with S, which can be produced greedily or using an
LP/ILP solver.
Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
number of edges in the subproblem s.
X
X
X
(CC1 )
(CC2 ) :
min
−φvi vj (1 − xvi vj ) +
φ vi vj x vi vj +
Q(φ, s, x),
x∈{0,1}|E|

(vi ,vj )∈E −

(vi ,vj )∈E +

s∈S

(CC2 )
where Q(φ, s, x) is defined as follows.
Q(φ, s, x)

=

min
s

x ∈{0,1}

s.t.

X
|s|

−φvi vj (1 − xsvi vj ) +

(vi ,vj )∈Es−

X

X

φvi vj xsvi vj

(2)

(vi ,vj )∈E +

xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

(vi ,vj )∈Ec+

We now construct a solution x∗ = {x∗vi vj , (xs∗
vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
as follows.
M

x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E + .

(3)
(4)

The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
vi vj and
is defined as follows.

1, if (vi , vj ) ∈ Es−
xs∗
=
(5)
vi vj
0, otherwise.
In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
vi vj )s∈S } is no greater than that of
{xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for all
s ∈ S.
It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
all s ∈ S.
Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
connected components that are adjacent to one another, not including the root node vs . Note, that
merging any pair of such components, does not increase the cost, since those components are not
separated by negative weight edges in subproblem s and so the result is still a partition.
Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
labeling formulation of min-cut, with the notation below.
4

We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
(
1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
s
f vi vj =
(6)
1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for all
(vi , vj ) ∈ Es− .
Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
X
X
Q(φ, s, x) = smin
φvi vj fvsi vj −
φvs v fvss v
(7)
fv v ≥0
i j
(vi ,vj )∈E +
mv ≥0

(vs ,v)∈Es−

λ−
vi vj

:

mvi − mvj ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

λ+
vi vj

:

mvj − mvi ≤ xvi vj + fvsi vj

∀(vi , vj ) ∈ (E + \ Es+ ),

ψv−

:

xvs v − fvss v ≤ mv

∀(vs , v) ∈ Es− ,

ψv+

:

mv ≤ xvs v + fvss v

∀(vs , v) ∈ Es+ ,

This yields to the corresponding dual subproblem.
X
+
max −
(λ−
vi vj + λvi vj )xvi vj +
λ≥0
ψ≥0

s.t.

ψv+i

X

ψv− xvs v −

(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

X

ψv+ xvs v

(8)

(vs ,v)∈Es+

1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
X

X

+
(λ−
vi vj − λvi vj ) +

vj
(vi ,vj )∈(E + \Es+ )

φ vi vj

−
(λ+
vj vi − λvj vi ) ≥ 0

∀vi ∈ V − vs

vj
(vj ,vi )∈(E + \Es+ )

−φvs v − ψv− ≥ 0
φvs v − ψv+ ≥ 0
+
− (λ−
vi v j + λ vi vj ) ≥ 0

∀(vs , v) ∈ Es−
∀(vs , v) ∈ Es+
∀(vi , vj ) ∈ (E + \ Es+ ).

In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
xvi vj term.

+
−(λ−
if (vi , vj ) ∈ (E + \ Es+ )

vi vj + λvi vj ),




−ψv+j ,
if (vi , vj ) ∈ Es+
ωvzi vj =

ψv−j ,
if (vi , vj ) ∈ Es−




0,
if (vi , vj ) ∈ (E − \ Es− ).
We denote the set of all dual feasible solutions across s P
∈ S as Z, with z ∈ Z. Observe, that to
enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
We formulate CC as optimization using Z below.
X
X
(CC2 )
(CC3 ) = min
φ vi vj x vi vj −
(1 − xvi vj )φvi vj
(CC3 )
x∈{0,1}|E|

s.t.

X

(vi ,vj )∈E −

(vi ,vj )∈E +

xvi vj ωvzi vj ≤ 0

∀z ∈ Z

(vi ,vj )∈E

5

Algorithm 1 Benders Decomposition for CC (BDCC)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

4.1

Ẑ = {}
done_LP = False
repeat
x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
did_add = False
for s ∈ S do
if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj ) < xvi vj then
z1 = Get Benders row via Eq (8).
z2 = Get MWR via Sec. 5.
Ẑ = Ẑ ∪ z1 ∪ z2
did_add = True
end if
end for
if did_add=False then
done_LP = True
end if
until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
Return x

Cutting Plane Optimization

Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
master problem) and generating new Benders rows until no violated constraints exist.
This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj , then
we have identified a violated cycle inequality associated with s.
We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
rows, which we describe in the following Sec. 5.
Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
1
provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
vi vj = 1, if xvi vj > 2
∗
and otherwise set x∗∗
vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
∗∗
connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
(supplementary material), we provide a more involved approach to produce feasible integer solutions.
In this section, we characterized CC using Benders decomposition and provided a cutting plane
algorithm to solve the corresponding optimization.

5

Magnanti-Wong Benders Rows

We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
6

Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
and black for not using Magnanti-Wong rows. We show both the computation time with and without
exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
total running time versus the total running time when solving each subproblem is done on its own
CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
where we replace the objective and add one additional constraint.
We follow the tradition of the operations research literature and use a random negative valued vector
(with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
−1
subproblem is solved. We experimented with using as an objective .0001+|φ
, which encourages
vi vj |
the cutting of edges with large positive weight, but it works as well as the random negative objective.
Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
X
X
X
+
τ Q(φ, s, x) ≤ −
(λ−
ψv− xvs v −
ψv+ xvs v
vi vj + λvi vj )xvi vj +
(vs ,v)∈Es−

(vi ,vj )∈(E + \Es+ )

(vs ,v)∈Es+

(9)
Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
experiments, we found that τ = 12 provides strong performance.

6

Experiments: Image Segmentation

In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC successfully exploits parallelization; (3) the use of MWR dramatically accelerates
optimization.
To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
linear and integer linear programming problems considered during the course of optimization. We use
a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
7

Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
MWR are generated.
=0.1

=1

=10

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.266
0.0426

0.372
0.0532
0.777
0.0745

0.585
0.0745
0.904
0.0745

0.894
0.106
0.968
0.138

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.149
0.0106
0.319
0.0532

0.394
0.0638
0.819
0.0745

0.606
0.0745
0.947
0.106

0.904
0.16
0.979
0.17

τ

par

10

50

100

300

0.5
0
0.5
0

0
0
1
1

0.202
0.0532
0.447
0.0638

0.426
0.0957
0.936
0.128

0.628
0.128
0.979
0.181

0.915
0.223
0.989
0.287

We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
minimum vertex cover problem can be solved approximately or greedily.
In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
the exact value of τ does not effect the speed of optimization dramatically. We show performance
with and without relying on parallel processing. Our parallel processing times assume that we
have one CPU for each subproblem. For the problem instances in our application the number of
subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time comparisons share only the time to solve the master problem. We observe large benefits
of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
improvement, since the master problem consumes a larger proportion of total CPU time.
In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
instances, the total CPU time required when using no MWR was prohibitively large, which is not the
case when MWR are employed. Thus most problem instances solved without MWR terminated early.
In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
the difference between the anytime solution (upper bound) and the lower bound on the objective. For
each such tolerance , we compute the percentage of instances, for which the duality gap is less than
, after various amounts of time. We observe that the performance of optimization without MWR,
but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
demonstrates that, across the dataset, MWR are of greater importance than parallelization.

7

</corps> 
<conclusion>Conclusions

We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
expect will find further use in the application of combinatorial optimization to problems in computer
vision.
8

The exploitation of results from the domain of operations research may lead to improved variants
of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
problem so as to enforce integrality on a subset of the variables of the master problem.

</conclusion>
<acknowledgement></acknowledgement> 
<references>References
[1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
(ICCV-11), pages 2611–2618, 2011.
[2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
International Conference on Computer Vision (ECCV-12), 2012.
[3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister. Segmenting
planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
[4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
[5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
238–247, 2002.
[6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
[7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
solver for multicut partitioning. In CVPR, 2014.
[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
[9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
10.1007/978-3-319-46475-6_44.
[10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.
[11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations
research, 33(5):989–1007, 1985.
[12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous aircraft
routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
[13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
1776–1781, 1966.
[14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
399–404, 1956.
[15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
Management science, 20(5):822–844, 1974.
[16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
Research (volume 9), 1961.
[17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
Springer, 2016.
[18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
[19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
2015.

9

[20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
image and mesh graphs by lifted multicuts. In ICCV, 2015.
[21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image segmentation.
In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
[22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
Mathematical Programming Computation, 1(1):43–67, 2009.
[23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
model selection criteria. Operations research, 29(3):464–484, 1981.
[24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
[25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
pages 769–776. ACM, 2009.
[26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
http://dl.acm.org/citation.cfm?id=2969239.2969249.
[27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut:
Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
[28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
june 2007.
[29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
IEEE Transactions on, 39(5):694–697, May 1990.
[30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
CVPR, 2017.
[31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
CVPR, 2015.
[32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
[33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 652–666, 2018.
[34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
[35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
Systems, 2015.
[36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
preprint arXiv:1805.04958, 2018.
[37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
[38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual decomposition.
In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
[39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
microscopy. In ISBI, 2014.

10

A

APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ ) = 0.
Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
vi vj )s∈S } is constructed, for which
Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms of
xs .
M

x∗vi vj = xvi vj + max xsvi vj
s∈S

∀(vi , vj ) ∈ E +

M

x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
M

∀(vi , vj ) ∈ E +

M

∀(vi , vj ) ∈ Es− , s ∈ S.

xs∗
vi vj = 0
xs∗
vi vj = 1

(10)

The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond to the
optimizing solution for f in subproblem s, given x, x∗ respectively.
x∗vi vj = xvi vj + max fvsi vj
s∈S

x∗vi vj = xvi vj − fvsi vj

∀(vi , vj ) ∈ E +

∀(vi , vj ) ∈ Es− , s ∈ S

fvs∗
= 0 ∀(vi , vj ) ∈ E +
i vj

(11)

fvs∗
= 0 ∀(vi , vj ) ∈ Es−
i vj
These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice, that
since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq. (10),
which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the total
P
decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than the
former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other hand
the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero, yields
in an increase of the objective of the master problem by −φvi vj (1 − xn
vi vj ), while the objective of subproblem
s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

B

Line by Line Description of BDCC

We provide the line by line description of Alg. 1.
• Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
• Line 2: Indicate that we have not solved the LP relaxation yet.
• Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
integral solution is produced.
1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
indicated by done_lp=True.
2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to subproblems, associated with violated cycle inequalities.
– Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
– Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
set Ẑ.
– Line 11: Indicate that a Benders row was added this iteration.
4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
solving the master problem for the remainder of the algorithm.
• Line 18 Return solution x.

11

C

Generating Feasible Integer Solutions Prior to Convergence

Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand. This is so
that a practitioner can terminate optimization, when the gap between the objectives of the integral solution and
the relaxation is small. In this section we consider the production of feasible integer solutions, given the current
solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to this
procedure as rounding.
Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
using x∗ below.
κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
κvi vj =

φvi vj x∗vi vj

∀(vi , vj ) ∈ E

(12)

−

∗

Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle inequalities). Let
xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
vi vj = 1 if exactly
one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0, where
s∗
x0s
as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
vi vj = 1Es− (vi , vj ), is achieved using x
s∗
The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
then the solution produced below has cost equal to that of x∗ .
M

xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
M

s∗
x+
vi vj = max xvi vj
s∈S

M

s∗
x+
vi vj = xvi vj

∀(vi , vj ) ∈ E +

(13)

∀(vi , vj ) ∈ Es− , s ∈ S

The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is close to
integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by design of κ.
We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition x+
by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the cutting of
edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
x∗ )
1: x+
vi vj = 0 ∀(vi , vj ) ∈ E
2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
4: for s ∈ S do
5:
xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
+
s
6:
x+
vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
+
7:
κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
8: end for
9: Return x+
• Line 1: Initialize x+ as the zero vector.
• Line 2-3: Set κ according to Eq. (12)
• Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
2. Line 6: Cut edges in x+ that are cut in xs .
3. Line 7: Set φvi vj to zero for cut edges in x+ .
• Line 9: Return the solution x+
When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al. [28],
though we do not exploit its capacity to tackle non-submodular problems.

12

</references>
<discussion></discussion>